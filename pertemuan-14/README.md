<!--START_SECTION:medium-->
[Baca di Medium](https://medium.com/@dikaelsaputra/advanced-machine-learning-using-spark-mllib-1e5fe3e00ae2?source=rss-272e0aace4a6------2)

[Baca di Medium](https://medium.com/@dikaelsaputra/advanced-machine-learning-using-spark-mllib-1e5fe3e00ae2?source=rss-272e0aace4a6------2)

<figure><img alt="" src="https://cdn-images-1.medium.com/max/815/1*im7-WrTtooYDAEV_Swc6TQ.png" /></figure><p>Dalam dunia big data dan komputasi terdistribusi, Apache Spark telah menjadi salah satu framework yang paling banyak digunakan. Spark menawarkan mesin yang kuat untuk memproses dataset dalam skala besar dan menyediakan berbagai pustaka untuk berbagai tugas analisis data. Salah satu pustaka paling penting dalam ekosistem Spark adalah Spark MLlib, yang didedikasikan untuk pembelajaran mesin (machine learning). MLlib adalah pustaka yang dapat diskalakan dan dirancang untuk pembelajaran mesin yang efisien, mendukung berbagai algoritma dan alat, termasuk klasifikasi, regresi, klasterisasi, dan pemfilteran kolaboratif.</p><p>Integrasi Spark MLlib dengan Apache Spark menjadikannya pilihan ideal untuk menangani dataset besar di lingkungan terdistribusi. Baik untuk pembuatan model prediktif atau klasterisasi data, Spark MLlib dapat digunakan untuk mengembangkan model pembelajaran mesin dengan cara yang sangat efisien, memanfaatkan seluruh kekuatan komputasi cluster.</p><p>Dalam blog ini, kami akan menunjukkan contoh praktis dari beberapa teknik pembelajaran mesin dasar menggunakan Spark MLlib, termasuk regresi linear, regresi logistik, dan klasterisasi menggunakan KMeans.</p><h3>Linear Regression with Spark MLlib</h3><p>Regresi linear adalah salah satu teknik yang paling banyak digunakan untuk memprediksi variabel output kontinu berdasarkan satu atau lebih fitur input. Mari kita lihat bagaimana cara mengimplementasikan model regresi linear yang sederhana di Spark MLlib.</p><h4>Step 1: Inisialisasi Spark Session</h4><p>Pertama, kita perlu menginisialisasi sesi Spark. Ini adalah titik masuk untuk menggunakan fungsionalitas Spark.</p><pre>from pyspark.sql import SparkSession<br />from pyspark.ml.regression import LinearRegression<br />from pyspark.ml.feature import VectorAssembler<br /><br /># Inisialisasi Spark Session<br />spark = SparkSession.builder.appName('MLlib Example').getOrCreate()</pre><ul><li>Membuat sebuah <em>Spark Session</em> dengan nama aplikasi 'MLlib Example'.</li><li>Spark Session ini memungkinkan pengguna untuk memproses data menggunakan Spark, termasuk library MLlib untuk <em>machine learning</em>.</li></ul><h4>Step 2: Memuat Data Contoh</h4><p>Kami akan memuat beberapa data contoh yang mencakup fitur dan variabel target untuk pelatihan.</p><pre>data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]<br />columns = ['ID', 'Feature', 'Target']<br />df = spark.createDataFrame(data, columns)</pre><ul><li>Dataset kecil dalam bentuk list Python diubah menjadi DataFrame Spark.</li><li>Dataset ini memiliki kolom:</li></ul><pre>ID: Penanda untuk setiap baris data.<br />Feature: Variabel independen.<br />Target: Variabel dependen yang akan diprediksi.</pre><h4>Step 3: Mempersiapkan Data untuk Pemodelan</h4><p>Selanjutnya, kita mempersiapkan data untuk pemodelan. Karena Spark mengharuskan fitur input dalam format vektor, kita akan menggunakan VectorAssembler untuk mengonversi kolom fitur tunggal menjadi vektor.</p><pre>assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')<br />df_transformed = assembler.transform(df)</pre><ul><li><strong>VectorAssembler</strong> digunakan untuk menggabungkan kolom-kolom fitur menjadi satu kolom vektor bernama Features.</li><li>Dalam hal ini, hanya ada satu fitur (Feature), sehingga kolom Features akan menjadi representasi vektor untuk setiap baris.</li></ul><h4>Step 4: Melatih Model Regresi Linear</h4><p>Sekarang, kita akan melatih model regresi linear menggunakan dataset yang telah ditransformasi.</p><pre>lr = LinearRegression(featuresCol='Features', labelCol='Target')<br />model = lr.fit(df_transformed)</pre><ul><li>Membuat model regresi linier dengan parameter:</li></ul><pre>featuresCol: Nama kolom yang berisi vektor fitur (Features).<br />labelCol: Nama kolom target atau label (Target).</pre><ul><li>Model dilatih menggunakan dataset yang telah ditransformasi.</li></ul><h4>Step 5: Melihat Koefisien Model</h4><p>Terakhir, kita bisa mencetak koefisien model untuk melihat hasil regresi linear.</p><pre>print(f'Koefisien: {model.coefficients}')<br />print(f'Intercept: {model.intercept}')</pre><ul><li><strong>Coefficients</strong>: Nilai slope (kemiringan) dari regresi linier.</li><li><strong>Intercept</strong>: Titik potong regresi linier pada sumbu y.</li></ul><p><strong>Output:</strong></p><pre>Coefficients: [0.9999999999999992]<br />Intercept: 15.000000000000009</pre><p><strong>Koefisien (Coefficients): </strong><strong>[0.9999999999999992]</strong></p><ul><li>Nilai ini adalah <em>slope</em> (kemiringan) dari garis regresi.</li><li>Artinya, setiap peningkatan 1 unit pada Feature akan menyebabkan peningkatan hampir <strong>1 unit</strong> pada Target.</li><li>Nilai ini sangat mendekati 1, menunjukkan hubungan linier yang hampir sempurna.</li></ul><p><strong>Intercept: </strong><strong>15.000000000000009</strong></p><ul><li>Ini adalah titik potong garis regresi dengan sumbu Y.</li><li>Saat Feature = 0, nilai prediksi Target adalah sekitar <strong>15</strong>.</li></ul><p>Hasil prediksi dari model sangat sesuai dengan data asli, menunjukkan model ini sangat cocok (perfect fit) untuk dataset kecil yang digunakan. Model ini mengungkapkan hubungan linier sederhana antara Feature dan Target, di mana:</p><ul><li>Target bertambah 1 untuk setiap penambahan 1 pada Feature.</li><li>Model ini cocok untuk dataset kecil dengan hubungan linier yang jelas.</li></ul><h3>Logistic Regression with Spark MLlib</h3><p>Regresi logistik banyak digunakan untuk tugas klasifikasi biner. Teknik ini membantu memprediksi hasil biner berdasarkan satu atau lebih fitur input.</p><h4>Step 1: Memuat Data Contoh</h4><p>Untuk contoh ini, kami membuat dataset kecil di mana setiap baris berisi fitur dan label biner (0 atau 1).</p><pre>from pyspark.ml.classification import LogisticRegression<br />from pyspark.ml.linalg import Vector<br /><br />data = [(1, Vectors.dense([2.0, 3.0]), 0), (2, Vectors.dense([1.0, 5.0]), 1), (3, Vectors.dense([2.5, 4.5]), 1), (4, Vectors.dense([3.0, 6.0]), 0)]<br />columns = ['ID', 'Features', 'Label']<br />df = spark.createDataFrame(data, columns)</pre><ul><li>Dataset terdiri dari:</li></ul><pre>ID: Identitas unik setiap data.<br />Features: Vektor fitur untuk klasifikasi.<br />Label: Nilai target (0 atau 1), yang merepresentasikan dua kelas yang berbeda.</pre><ul><li>Fitur menggunakan Vectors.dense untuk merepresentasikan vektor numerik.</li></ul><h4>Step 2: Melatih Model Regresi Logistik</h4><p>Sekarang, kita akan melatih model regresi logistik dengan data yang diberikan.</p><pre>lr = LogisticRegression(featuresCol='Features', labelCol='Label')<br />model = lr.fit(df)</pre><ul><li>Membuat model Logistic Regression dengan parameter:</li></ul><pre>featuresCol='Features': Kolom yang berisi fitur.<br />labelCol='Label': Kolom yang berisi label target.</pre><ul><li>Model dilatih untuk memprediksi probabilitas kelas Label berdasarkan Features.</li></ul><h4>Step 3: Melihat Koefisien dan Intercept Model</h4><p>Setelah model dilatih, kita dapat melihat koefisien dan intercept yang membantu memahami hubungan antara fitur dan kelas yang diprediksi.</p><pre>print(f'Koefisien: {model.coefficients}')<br />print(f'Intercept: {model.intercept}')</pre><ul><li><strong>Coefficients</strong>: Menunjukkan kontribusi masing-masing fitur dalam menentukan probabilitas kelas.</li><li><strong>Intercept</strong>: Titik potong fungsi logistik dengan sumbu probabilitas.</li></ul><p><strong>Output:</strong></p><pre>Coefficients: [-12.262057921492042,4.087352263925821]<br />Intercept: 11.568912718955037</pre><p>Hasil ini menunjukkan model Logistic Regression yang telah dilatih untuk memprediksi probabilitas dari dua kelas (0 dan 1). Berikut interpretasi dari hasil:</p><p><strong>Coefficients: </strong><strong>[-12.262057921492042, 4.087352263925821]</strong></p><ul><li>Ini adalah bobot (<em>weights</em>) untuk setiap fitur dalam menentukan kelas.</li><li>Fitur pertama (<strong>x1</strong>​) memiliki koefisien negatif besar: <strong>-12.262</strong>, menunjukkan bahwa nilai tinggi pada fitur ini mengurangi probabilitas sampel masuk ke kelas 1.</li><li>Fitur kedua (<strong>x2</strong>​) memiliki koefisien positif: <strong>4.087</strong>, menunjukkan bahwa nilai tinggi pada fitur ini meningkatkan probabilitas sampel masuk ke kelas 1.</li></ul><p><strong>Intercept: </strong><strong>11.568912718955037</strong></p><ul><li>Ini adalah bias atau nilai awal sebelum mempertimbangkan kontribusi dari fitur.</li><li>Nilai ini menunjukkan dasar probabilitas awal untuk kelas 1, sebelum fitur diterapkan.</li></ul><p><strong>Interpretasi Praktis</strong></p><ul><li>Jika <strong>x1</strong>​ sangat besar dibandingkan <strong>x2</strong>​, probabilitas kelas 1 akan turun drastis karena pengaruh negatif dari <strong>x1</strong>​.</li><li>Jika <strong>x2</strong> sangat besar atau dominan, probabilitas kelas 1 meningkat, karena kontribusi positif dari <strong>x2</strong>​.</li><li>Intercept positif menunjukkan bahwa dalam kondisi awal (tanpa fitur), model cenderung memprediksi kelas 1.</li></ul><p>Artinya, untuk sampel ini, model memprediksi kelas 0 (probabilitas < 0.5).</p><p>Kesimpulannya Koefisien dan intercept mencerminkan kontribusi masing-masing fitur terhadap probabilitas kelas 1. Fitur pertama (<strong>x1</strong>​) memiliki efek negatif yang kuat terhadap prediksi kelas 1. Fitur kedua (<strong>x2</strong>​) memberikan efek positif untuk meningkatkan probabilitas kelas 1. Model ini cocok untuk menganalisis pengaruh fitur dalam klasifikasi biner.</p><h3>KMeans Clustering with Spark MLlib</h3><p>KMeans clustering adalah algoritma pembelajaran tidak terawasi yang populer yang digunakan untuk mengelompokkan data ke dalam cluster berdasarkan fitur mereka. Mari kita lihat cara melakukan klasterisasi dengan Spark MLlib.</p><h4>Step 1: Memuat Data Contoh</h4><p>Kita akan menggunakan dataset sederhana di mana setiap baris berisi vektor fitur yang akan digunakan untuk membuat cluster.</p><pre>from pyspark.ml.clustering import KMeans<br /><br />data = [(1, Vectors.dense([1.0, 1.0])), (2, Vectors.dense([5.0, 5.0])), (3, Vectors.dense([10.0, 10.0])), (4, Vectors.dense([15.0, 15.0]))]<br />columns = ['ID', 'Features']<br />df = spark.createDataFrame(data, columns)</pre><ul><li>Dataset terdiri dari:</li></ul><pre>ID: Identitas unik setiap data.<br />Features: Vektor 2 dimensi ([x,y]) yang akan digunakan untuk klastering.</pre><ul><li>Empat titik data mewakili posisi di ruang dua dimensi.</li></ul><h4>Step 2: Melatih Model KMeans</h4><p>Sekarang, kita akan melatih model KMeans dengan data tersebut, menentukan jumlah cluster (k=2 pada contoh ini).</p><pre>kmeans = KMeans(featuresCol='Features', k=2)<br />model = kmeans.fit(df)</pre><ul><li>Membuat model KMeans dengan parameter:</li></ul><pre>featuresCol='Features': Kolom yang berisi fitur untuk klastering.<br />k=2: Jumlah cluster yang ingin dibentuk.</pre><ul><li>Model dilatih untuk mengelompokkan data ke dalam dua cluster.</li></ul><h4>Step 3: Melihat Pusat Cluster</h4><p>Setelah model dilatih, kita bisa memeriksa pusat cluster, yang mewakili centroid dari cluster yang terbentuk.</p><pre>centers = model.clusterCenters()<br />print(f'Pusat Cluster: {centers}')</pre><ul><li><strong>Cluster Centers</strong>: Koordinat pusat dari masing-masing cluster di ruang fitur.</li><li>Pusat cluster dihitung sebagai rata-rata semua data yang termasuk dalam cluster tersebut.</li></ul><p><strong>Output:</strong></p><pre>Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]</pre><p><strong>Cluster Center 1: </strong><strong>[5.33333333, 5.33333333]</strong></p><ul><li>Pusat cluster ini dihitung sebagai rata-rata koordinat dari data yang tergabung dalam cluster tersebut.</li><li>Data yang termasuk ke dalam cluster ini adalah:</li></ul><pre>[1.0, 1.0]<br />[5.0, 5.0]<br />[10.0, 10.0]</pre><p>Perhitungannya:</p><a href="https://medium.com/media/e04608b09bda9f27ac83407536372edc/href">https://medium.com/media/e04608b09bda9f27ac83407536372edc/href</a><p><strong>Cluster Center 2: </strong><strong>[15.0, 15.0]</strong></p><ul><li>Pusat cluster ini dihitung dari satu data yang berada jauh dari yang lain, yaitu [15.0, 15.0].</li><li>Karena hanya satu data yang berada dalam cluster ini, pusat clusternya adalah koordinat data tersebut.</li></ul><h4><strong>Interpretasi Hasil</strong></h4><p><strong>Distribusi Cluster</strong>:</p><ul><li><strong>Cluster 1</strong> mencakup tiga titik ([1.0, 1.0], [5.0, 5.0], dan [10.0, 10.0]), karena jarak antar titik ini relatif lebih kecil dibandingkan jaraknya ke [15.0, 15.0].</li><li><strong>Cluster 2</strong> hanya mencakup satu titik ([15.0, 15.0]) karena jaraknya jauh dari tiga titik lainnya.</li></ul><p><strong>Cluster Centers</strong>:</p><ul><li>Cluster Center 1 ([5.333, 5.333]) berada di antara data pada cluster pertama, mencerminkan distribusi rata-rata data di cluster tersebut.</li><li>Cluster Center 2 ([15.0, 15.0]) hanya merepresentasikan data tunggal karena data itu tidak memiliki tetangga dekat.</li></ul><p>Kesimpulannnya <strong>KMeans</strong> mengelompokkan data berdasarkan jarak Euclidean, sehingga data dengan kedekatan jarak yang lebih besar cenderung membentuk cluster bersama. Cluster kedua hanya memiliki satu titik karena data [15.0, 15.0] berada jauh dari data lainnya. Algoritma ini cocok digunakan untuk data yang terdistribusi secara jelas, tetapi jika data memiliki distribusi yang tidak seimbang, parameter <strong><em>k</em></strong> atau preprocessing data perlu dioptimalkan untuk mendapatkan hasil yang lebih baik.</p><h3>Homework</h3><ul><li>Load a real-world dataset into Spark and prepare it for machine learning tasks.</li><li>Build a classification model using Spark MLlib and evaluate its performance.</li><li>Explore hyperparameter tuning using cross-validation.</li></ul><h3>LogisticRegression</h3><h4>Persiapan Spark dan Dataset</h4><pre>from pyspark.sql import SparkSession<br />import pandas as pd<br /><br />spark = SparkSession.builder.appName("TitanicPrediction").getOrCreate()<br /><br /># Load dataset dari URL menggunakan pandas<br />url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"<br />titanic_pd = pd.read_csv(url)<br /><br /># Data Cleaning: Menangani nilai yang hilang<br />threshold = 0.5 * len(titanic_pd)<br />titanic_pd.dropna(thresh=threshold, axis=1, inplace=True)<br />titanic_pd['Age'] = titanic_pd['Age'].fillna(titanic_pd['Age'].mean())<br />titanic_pd['Embarked'] = titanic_pd['Embarked'].fillna('S')<br /><br /># Konversi pandas DataFrame ke PySpark DataFrame<br />titanic_spark = spark.createDataFrame(titanic_pd)<br /><br />print("Nilai yang hilang per kolom:\n", titanic_pd.isnull().sum())</pre><p>Pada bagian ini, kita memulai dengan inisialisasi SparkSession dan mengunduh dataset Titanic dari URL. Dataset ini kemudian dibersihkan dengan menghapus kolom dengan lebih dari 50% nilai yang hilang dan mengisi nilai-nilai yang hilang pada kolom Age dan Embarked dengan rata-rata dan 'S' (standard) yang sesuai.</p><p><strong>Output:</strong></p><pre>Nilai yang hilang per kolom:<br /> PassengerId    0<br />Survived       0<br />Pclass         0<br />Name           0<br />Sex            0<br />Age            0<br />SibSp          0<br />Parch          0<br />Ticket         0<br />Fare           0<br />Embarked       0<br />dtype: int64</pre><h4>Transformasi Data dengan StringIndexer dan OneHotEncoder</h4><pre>from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler<br /><br />indexers = [<br />    StringIndexer(inputCol="Sex", outputCol="Sex_Index"),<br />    StringIndexer(inputCol="Embarked", outputCol="Embarked_Index")<br />]<br /><br />encoders = [<br />    OneHotEncoder(inputCol="Sex_Index", outputCol="Sex_Encoded"),<br />    OneHotEncoder(inputCol="Embarked_Index", outputCol="Embarked_Encoded")<br />]<br /><br />feature_cols = ["Pclass", "Sex_Encoded", "Age", "SibSp", "Parch", "Fare", "Embarked_Encoded"]<br /><br />assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")</pre><p>Pada bagian ini, StringIndexer digunakan untuk mengonversi nilai kategori seperti Sex dan Embarked menjadi indeks numerik. Kemudian, OneHotEncoder mengonversi indeks numerik ini menjadi vektor biner (one-hot encoding) untuk fitur-fitur tersebut. Kolom features adalah kombinasi dari semua fitur yang akan digunakan untuk pelatihan model.</p><h4>Definisi dan Inisialisasi Logistic Regression Model</h4><pre>from pyspark.ml.classification import LogisticRegression<br /><br />lr = LogisticRegression(featuresCol="features", labelCol="Survived",<br />                        aggregationDepth=2, elasticNetParam=0.0, family='auto', <br />                        fitIntercept=True, maxBlockSizeInMB=0.0, maxIter=100,<br />                        predictionCol="prediction", probabilityCol="probability",<br />                        rawPredictionCol="rawPrediction", regParam=0.01, <br />                        standardization=True, threshold=0.5, tol=1e-06)</pre><p>Model LogisticRegression diinisialisasi dengan berbagai parameter yang mempengaruhi cara model dilatih dan bagaimana ia memperlakukan data. Misalnya, regParam mengatur parameter regularisasi, elasticNetParam mengatur proporsi antara L1 dan L2 regularisasi, dan standardization menstandarisasi fitur sebelum pelatihan untuk meningkatkan akurasi model.</p><h4>Pipelinisasi Transformasi Data dan Pelatihan Model</h4><pre>from pyspark.ml import Pipeline<br /><br />pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])</pre><p>Pipeline digunakan untuk menggabungkan berbagai langkah transformasi dan pelatihan model menjadi satu proses terintegrasi. Ini membantu memastikan bahwa setiap langkah, dari preprocessing hingga pelatihan model, dilakukan dengan urutan yang benar.</p><h4>Pembagian Data Latih dan Uji</h4><pre>train_data, test_data = titanic_spark.randomSplit([0.8, 0.2], seed=123)</pre><p>Data dibagi menjadi dua bagian: 80% untuk pelatihan (training) dan 20% untuk pengujian (testing). Pembagian ini dilakukan dengan menggunakan fungsi randomSplit dari PySpark.</p><h4>Grid Search untuk Hyperparameter Tuning</h4><pre>from pyspark.ml.tuning import ParamGridBuilder, CrossValidator<br />from pyspark.ml.evaluation import BinaryClassificationEvaluator<br /><br />param_grid = ParamGridBuilder() \<br />    .addGrid(lr.regParam, [0.01, 0.1, 1, 10, 100]) \<br />    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \<br />    .build()<br /><br />evaluator = BinaryClassificationEvaluator(labelCol="Survived", metricName="areaUnderROC")<br />cross_val = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)</pre><p>Grid Search digunakan untuk mencari set parameter terbaik untuk model LogisticRegression dengan cara menguji kombinasi parameter yang berbeda. CrossValidator diterapkan untuk mengukur kinerja model dengan validasi silang 5-kali lipat.</p><h4>Pelatihan Model dengan Cross-Validation dan Evaluasi</h4><pre>cv_model = cross_val.fit(train_data)<br />best_model = cv_model.bestModel<br /><br />predictions = best_model.transform(test_data)</pre><p>Model dilatih menggunakan data latih dan parameter terbaik yang ditemukan melalui Grid Search. Model terbaik yang ditemukan disimpan sebagai best_model dan diterapkan untuk memprediksi data uji.</p><h4>Evaluasi Model dengan ROC Curve dan AUC</h4><pre>from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score<br /><br />auc = evaluator.evaluate(predictions)<br />print("Area Under ROC:", auc)<br /><br />conf_matrix = confusion_matrix(predictions.select('Survived').collect(), predictions.select('prediction').collect())<br />plt.figure(figsize=(8, 6))<br />sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])<br />plt.title('Confusion Matrix')<br />plt.xlabel('Predicted')<br />plt.ylabel('Actual')<br />plt.show()<br /><br />y_true = predictions.select('Survived').rdd.flatMap(lambda x: x).collect()<br />y_proba = predictions.select('probability').rdd.flatMap(lambda x: x).collect()<br />y_proba = [p[1] for p in y_proba]<br />fpr, tpr, _ = roc_curve(y_true, y_proba)<br />roc_auc = roc_auc_score(y_true, y_proba)<br /><br />plt.figure(figsize=(8, 6))<br />plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')<br />plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier', color='red')<br />plt.title('ROC Curve')<br />plt.xlabel('False Positive Rate')<br />plt.ylabel('True Positive Rate')<br />plt.legend(loc='lower right')<br />plt.show()</pre><p>Model dievaluasi dengan menggunakan BinaryClassificationEvaluator untuk menghitung AUC (Area Under the Curve) dari ROC (Receiver Operating Characteristic) Curve. Selain itu, juga disertakan visualisasi Confusion Matrix dan ROC Curve untuk memahami kinerja model dengan lebih baik.</p><pre>Area Under ROC: 0.8426071878940733</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*dIzhR7gtJc7E2NbzvUYNFg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*lWCrryFibVYykJF84H1E6w.png" /></figure><h3>Kesimpulan</h3><p>Spark MLlib menyediakan solusi yang kuat dan dapat diskalakan untuk tugas pembelajaran mesin di lingkungan terdistribusi. Baik Anda bekerja pada regresi, klasifikasi, atau klasterisasi, Spark MLlib menawarkan alat yang sederhana namun efektif untuk mengembangkan model pembelajaran mesin.</p><p>Dalam blog ini, kami telah menunjukkan bagaimana cara melakukan regresi linear, regresi logistik, dan klasterisasi KMeans menggunakan Spark MLlib. Contoh-contoh ini menjadi dasar untuk menerapkan teknik pembelajaran mesin yang lebih maju menggunakan kekuatan Spark, membantu Anda menangani dataset besar dengan cara yang efisien. Dengan memanfaatkan kemampuan pemrosesan terdistribusi Spark, Anda dapat menskalakan model pembelajaran mesin Anda untuk menangani tugas-tugas yang paling menuntut.</p><p>Dengan Spark MLlib, kemungkinan untuk membangun aplikasi pembelajaran mesin tingkat lanjut sangat luas, dan dengan kemudahan integrasinya ke dalam ekosistem Spark, ia tetap menjadi salah satu pilihan utama bagi ilmuwan data dan insinyur.</p><ul><li><a href="https://drive.google.com/file/d/18Tt6zqhQyKQ39ahnjwr3CmOUNX_QUOBo/view">Hands_On_Pertemuan_14.ipynb</a></li><li><a href="https://github.com/figuran04/big-data/tree/main/pertemuan-14">big-data/pertemuan-14 at main · figuran04/big-data</a></li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1e5fe3e00ae2" width="1" />
<!--END_SECTION:medium-->
