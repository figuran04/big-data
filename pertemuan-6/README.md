<!--START_SECTION:medium-->
[Baca di Medium](https://medium.com/@dikaelsaputra/data-processing-dengan-apache-spark-14af61b5c22b?source=rss-272e0aace4a6------2)

<figure><img alt="" src="https://cdn-images-1.medium.com/max/848/0*yHcF4iZDqGll-Hrm" /></figure><p>Apache Spark adalah platform komputasi terdistribusi yang sangat populer untuk pemrosesan data dalam skala besar. Salah satu modul utama dari Spark adalah <strong>Spark SQL</strong> yang memungkinkan pengguna untuk memproses data menggunakan konsep DataFrame. DataFrame mirip dengan tabel dalam database relasional yang memungkinkan pemrosesan data dengan skema dan tipe data yang terdefinisi. Pada sesi ini, kita akan membahas pembuatan DataFrame di PySpark, melakukan transformasi data, bekerja dengan tipe data kompleks, serta menerapkan operasi lanjutan menggunakan window function.</p><h3>Setup Lingkungan</h3><p>Sebelum kita memulai pastikan Anda mengikuti seri ini <a href="https://medium.com/@dikaelsaputra/panduan-lengkap-pyspark-dan-pandas-instalasi-praktik-dasar-dan-lanjutan-86ab9ce4ea55">instalasi Anaconda</a>, pastikan semua paket yang dibutuhkan telah terinstal:</p><pre>pip install pyspark==3.4.1<br />pip install pandas<br />pip install findspark</pre><p>Jika pyspark tidak terdeteksi oleh IDE Anda, pastikan untuk menginisialisasi findspark agar pyspark dapat ditemukan:</p><pre>import findspark<br />findspark.init()</pre><h3>Membuat DataFrame Sederhana</h3><p>Langkah pertama adalah membuat <em>session</em> Spark dan membuat DataFrame sederhana untuk memulai eksplorasi data:</p><pre>from pyspark.sql import SparkSession<br /><br />spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()<br />data = [('James', 'Sales', 3000),<br />        ('Michael', 'Sales', 4600),<br />        ('Robert', 'Sales', 4100),<br />        ('Maria', 'Finance', 3000)]<br />columns = ['EmployeeName', 'Department', 'Salary']<br />df = spark.createDataFrame(data, schema=columns)<br />df.show()</pre><p>Output:</p><pre>| EmployeeName | Department | Salary |<br />|--------------|------------|--------|<br />| James        | Sales      | 3000   |<br />| Michael      | Sales      | 4600   |<br />| Robert       | Sales      | 4100   |<br />| Maria        | Finance    | 3000   |</pre><h3>Transformasi Dasar dengan DataFrames</h3><p>Pada tugas ini, kita akan melakukan beberapa transformasi data pada DataFrame yang telah dibuat menggunakan operasi dasar seperti filter, select, dan groupBy. Kita juga akan menambahkan operasi agregasi lainnya seperti count, sum, dan max untuk mendapatkan wawasan lebih banyak dari dataset.</p><pre>df.select('EmployeeName', 'Salary').show()<br />df.filter(df['Salary'] > 3000).show()<br />df.groupBy('Department').avg('Salary').show()<br />df.groupBy('Department').count().show()<br />df.groupBy('Department').sum('Salary').show()<br />df.groupBy('Department').max('Salary').show()</pre><p>Output:</p><p><strong>Menampilkan Kolom Tertentu</strong></p><pre>| EmployeeName | Salary |<br />|--------------|--------|<br />| James        | 3000   |<br />| Michael      | 4600   |<br />| Robert       | 4100   |<br />| Maria        | 3000   |</pre><p><strong>Filter Gaji di atas 3000</strong></p><pre>| EmployeeName | Department | Salary |<br />|--------------|------------|--------|<br />| Michael      | Sales      | 4600   |<br />| Robert       | Sales      | 4100   |</pre><p><strong>Rata-rata Gaji per Departemen</strong></p><pre>| Department | avg(Salary) |<br />|------------|-------------|<br />| Sales      | 3900.0      |<br />| Finance    | 3000.0      |</pre><p><strong>Jumlah Karyawan per Departemen</strong></p><pre>| Department | count |<br />|------------|-------|<br />| Sales      | 3     |<br />| Finance    | 1     |</pre><p><strong>Total Gaji per Departemen</strong></p><pre>| Department | sum(Salary) |<br />|------------|-------------|<br />| Sales      | 11700       |<br />| Finance    | 3000        |</pre><p><strong>Gaji Tertinggi per Departemen</strong></p><pre>| Department | max(Salary) |<br />|------------|-------------|<br />| Sales      | 4600        |<br />| Finance    | 3000        |</pre><h4>Penjelasan:</h4><ul><li><strong>select</strong> digunakan untuk memilih kolom EmployeeName dan Salary untuk menampilkan informasi sederhana.</li><li><strong>filter</strong> digunakan untuk menampilkan karyawan yang memiliki gaji di atas 3000.</li><li><strong>groupBy</strong> diikuti oleh avg, count, sum, dan max digunakan untuk menghitung rata-rata, jumlah karyawan, total gaji, dan gaji tertinggi berdasarkan departemen.</li></ul><h3>Bekerja dengan Tipe Data Kompleks</h3><p>Spark memungkinkan pengguna untuk melakukan operasi lebih kompleks seperti menambahkan kolom baru dan melakukan perhitungan berdasarkan kolom yang ada:</p><pre>df1 = df.withColumn('SalaryBonus', df['Salary'] * 0.1)<br />df1.show()<br />df2 = df1.withColumn('TotalCompensation', df1['Salary'] + df1['SalaryBonus'])<br />df2.show()</pre><p>Output:</p><pre>| EmployeeName | Department | Salary | SalaryBonus | TotalCompensation |<br />|--------------|------------|--------|-------------|------------------|<br />| James        | Sales      | 3000   | 300.0       | 3300.0           |<br />| Michael      | Sales      | 4600   | 460.0       | 5060.0           |<br />| Robert       | Sales      | 4100   | 410.0       | 4510.0           |<br />| Maria        | Finance    | 3000   | 300.0       | 3300.0           |</pre><p>Penambahan kolom ini dapat digunakan untuk menghitung bonus atau tunjangan lain yang mungkin diterima oleh karyawan berdasarkan gaji mereka.</p><h3>Operasi Data Lanjutan dengan Window Function</h3><p>Window function memungkinkan kita untuk melakukan operasi agregasi dan analitik yang lebih canggih, seperti menghitung ranking karyawan berdasarkan gaji di setiap departemen:</p><pre>from pyspark.sql.window import Window<br />from pyspark.sql import functions as F<br /><br />windowSpec = Window.partitionBy('Department').orderBy('Salary')<br />df2.withColumn('Rank', F.rank().over(windowSpec)).show()</pre><p>Ouput:</p><pre>| EmployeeName | Department | Salary | SalaryBonus | TotalCompensation | Rank |<br />|--------------|------------|--------|-------------|------------------|------|<br />| James        | Sales      | 3000   | 300.0       | 3300.0           | 1    |<br />| Robert       | Sales      | 4100   | 410.0       | 4510.0           | 2    |<br />| Michael      | Sales      | 4600   | 460.0       | 5060.0           | 3    |<br />| Maria        | Finance    | 3000   | 300.0       | 3300.0           | 1    |</pre><p>Pada materi <strong>“Data Processing dengan Apache Spark”</strong>, kita telah mempelajari berbagai operasi dasar hingga lanjutan dalam memanipulasi dan menganalisis data menggunakan PySpark. Berikut adalah ringkasan dari setiap tugas yang telah kita selesaikan serta bagaimana teknik-teknik tersebut dapat diterapkan pada proyek data Anda:</p><h3>Ringkasan Operasi yang Telah Dilakukan</h3><p><strong>Membuat DataFrame Sederhana:</strong></p><ul><li>Kita memulai dengan membuat DataFrame dari data statis dan melakukan eksplorasi data awal. Pembuatan DataFrame ini memudahkan kita dalam mengelola data terstruktur dan semi-terstruktur seperti CSV atau tabel SQL.</li><li><strong>Contoh Penerapan:</strong> DataFrame ini bisa digunakan untuk membaca dataset dari berbagai sumber data seperti HDFS, S3, atau Database Relasional.</li></ul><p><strong>Transformasi Data dengan Operasi Dasar:</strong></p><ul><li>Menggunakan operasi select untuk memilih kolom yang relevan, filter untuk menyaring data berdasarkan kondisi tertentu, dan groupBy untuk mengelompokkan serta melakukan agregasi data seperti menghitung rata-rata (avg), total (sum), jumlah (count), dan nilai maksimal (max).</li><li><strong>Contoh Penerapan:</strong> Teknik ini berguna untuk melakukan eksplorasi data awal, analisis data, dan persiapan data sebelum proses machine learning atau visualisasi. Misalnya, menghitung rata-rata pengeluaran per kategori, jumlah transaksi per bulan, atau distribusi penjualan berdasarkan wilayah.</li></ul><p><strong>Bekerja dengan Tipe Data Kompleks:</strong></p><ul><li>Membuat kolom baru menggunakan withColumn untuk melakukan kalkulasi tambahan, misalnya menghitung bonus berdasarkan gaji atau total kompensasi karyawan.</li><li><strong>Contoh Penerapan:</strong> Penerapan ini dapat digunakan untuk menghitung pendapatan tambahan, persentase pertumbuhan, atau analisis biaya yang memerlukan kalkulasi di level kolom tertentu.</li></ul><p><strong>Operasi Data Lanjutan dengan Window Function:</strong></p><ul><li>Menggunakan Window Function untuk melakukan perhitungan analitik yang lebih canggih seperti menghitung ranking, running total, atau rata-rata bergerak (<em>moving average</em>).</li><li><strong>Contoh Penerapan:</strong> Teknik ini bisa diterapkan pada analisis urutan, seperti menghitung ranking produk berdasarkan penjualan, peringkat karyawan berdasarkan kinerja, atau perhitungan rata-rata penjualan harian dalam rentang waktu tertentu.</li></ul><h3>Eksplorasi Teknik Lebih Lanjut</h3><p>Teknik yang telah kita pelajari ini merupakan fondasi dasar dalam pemrosesan data dengan Apache Spark. Berikut adalah beberapa teknik lanjutan yang dapat dieksplorasi lebih jauh untuk mengoptimalkan pemrosesan data Anda:</p><p><strong>Penggunaan Caching dan Persisting Data:</strong></p><ul><li>Spark menawarkan metode cache() dan persist() untuk menyimpan DataFrame sementara di memori. Teknik ini sangat bermanfaat saat Anda perlu melakukan beberapa operasi pada DataFrame yang sama sehingga mengurangi waktu eksekusi.</li></ul><p><strong>Partisi Data untuk Performa yang Lebih Baik:</strong></p><ul><li>Pembagian partisi data sangat penting saat bekerja dengan data dalam skala besar. Pengaturan partisi yang tepat dapat meningkatkan performa pemrosesan dan optimalisasi penggunaan sumber daya.</li></ul><p><strong>Integrasi dengan Hadoop Ecosystem:</strong></p><ul><li>Spark bisa diintegrasikan dengan HDFS, Hive, dan HBase untuk pemrosesan dan penyimpanan data yang lebih fleksibel. Misalnya, Anda dapat menggunakan Spark untuk membaca data dari HDFS, melakukan pemrosesan, dan menyimpan hasilnya kembali ke HDFS atau Hive sebagai tabel.</li></ul><p><strong>Penerapan UDF (User Defined Functions) untuk Transformasi Data yang Lebih Kompleks:</strong></p><ul><li>Anda dapat membuat UDF untuk menangani transformasi data yang tidak dapat dilakukan dengan fungsi bawaan Spark, seperti operasi pada string yang kompleks, pemrosesan data temporal, atau kalkulasi khusus yang memerlukan logika bisnis tertentu.</li></ul><p><strong>Penerapan Spark SQL dan Integrasi dengan Pandas:</strong></p><ul><li>Spark SQL memungkinkan pengguna untuk menulis query SQL pada DataFrame, yang sangat membantu pengguna dengan latar belakang SQL. Selain itu, integrasi dengan Pandas memungkinkan pengolahan data dalam skala kecil setelah data difilter dari Spark.</li></ul><h3>Contoh Penerapan Teknik dalam Proyek Data</h3><p>Sebagai contoh penerapan teknik-teknik ini pada proyek data, Anda bisa menggunakan Spark untuk:</p><ul><li><strong>Analisis Perilaku Pelanggan:</strong> Dengan memanfaatkan groupBy, agg, dan window function, Anda bisa menghitung perilaku pelanggan berdasarkan pembelian per bulan, segmentasi berdasarkan pengeluaran, dan analisis churn pelanggan.</li><li><strong>Optimalisasi Logistik:</strong> Dengan menghitung distribusi stok per wilayah, menghitung rata-rata waktu pengiriman, dan membuat perkiraan kebutuhan stok menggunakan moving average.</li><li><strong>Integrasi Data di Lingkungan Big Data:</strong> Menggunakan Spark untuk memproses data dari HDFS atau S3, lalu menyimpannya kembali setelah proses ETL (Extract, Transform, Load).</li></ul><p>Dengan pemahaman ini, Anda akan lebih siap untuk mengembangkan pipeline data yang lebih canggih, melakukan analitik lanjutan, dan mengoptimalkan pemrosesan data di lingkungan terdistribusi menggunakan Apache Spark.</p><h3>Referensi</h3><p><a href="https://drive.google.com/file/d/13f8mjOFzXZ7g-i0eVGsVQXCzG8iO0emZ/view?usp=sharing">Hands_On_Pertemuan_6.ipynb</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=14af61b5c22b" width="1" />
<!--END_SECTION:medium-->