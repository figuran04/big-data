<!--START_SECTION:medium-->
[Baca di Medium](https://medium.com/@dikaelsaputra/instalasi-dan-konfigurasi-hadoop-serta-spark-di-windows-f7f3582def93?source=rss-272e0aace4a6------2)

<h3>Instalasi dan Konfigurasi Hadoop serta Spark di Windows: Instalasi JDK Hingga Integrasi Hadoop dengan Spark</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*N4ybCsS4TUFsX5sU.jpg" /></figure><p>Hadoop dan Spark adalah dua komponen penting dalam dunia Big Data. Hadoop bertanggung jawab untuk penyimpanan dan pemrosesan data dalam skala besar, sementara Spark menawarkan kerangka kerja yang cepat dan mudah digunakan untuk analisis data. Bagi mereka yang ingin mulai bekerja dengan Big Data, menguasai instalasi dan konfigurasi Hadoop serta Spark di Windows adalah langkah awal yang krusial. Beberapa poin penting sebagai berikut:</p><ol><li><strong>Hadoop Distributed File System (HDFS):</strong> HDFS adalah sistem file terdistribusi yang memungkinkan penyimpanan data dalam skala besar di banyak mesin, sehingga memastikan redundansi dan ketersediaan tinggi.</li><li><strong>Spark:</strong> Spark dikenal dengan kecepatan dan kemampuannya untuk memproses data dalam memori, yang membuatnya jauh lebih cepat dibandingkan MapReduce tradisional di Hadoop.</li><li><strong>Winutils:</strong> Dalam lingkungan Windows, winutils.exe sangat penting agar Hadoop dapat berjalan dengan benar karena beberapa fungsi Hadoop memerlukan utilitas ini.</li><li><strong>Konfigurasi yang Tepat:</strong> Kesuksesan dalam instalasi dan konfigurasi sangat bergantung pada penyesuaian environment variables dan file konfigurasi Hadoop agar sesuai dengan sistem operasi dan arsitektur jaringan.</li></ol><p>Artikel ini akan memandu Anda melalui langkah-langkah instalasi JDK, Hadoop, dan Spark di sistem operasi Windows, serta cara mengintegrasikan Spark dengan Hadoop agar keduanya dapat berjalan secara efektif.</p><h3>Langkah-langkah Instalasi dan Konfigurasi</h3><h4>Download Resource yang Dibutuhkan</h4><ul><li><strong>JDK 11</strong>: <a href="https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11024/jdk-11.0.24_windows-x64_bin.exe">Download JDK 11</a></li><li><strong>Hadoop 3.3.6</strong>: <a href="https://hadoop.apache.org/releases.html">Download Hadoop</a></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*pPwzjuc9Ie9tcvD2fd9_Mg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/592/1*FVq7LwJ0aHQUiV5R7Hwwsw.png" /></figure><ul><li><strong>Spark 3.5.2</strong>: <a href="https://www.apache.org/dyn/closer.lua/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz">Download Spark</a></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*t4-SQPjwTuRPaP2ttGpqfw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/605/1*DwHPfOBscsVUZJ5kIyNr5g.png" /></figure><ul><li><strong>Winutils</strong>: <a href="https://github.com/cdarlint/winutils">Download Winutils</a></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*Rizg7t-hIK87uK5nO5cjpQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/597/1*AhXsE6KBujcXIeYvBtHykw.png" /></figure><h4>Instalasi JDK</h4><ul><li>Jalankan installer JDK 11 yang telah diunduh.</li><li>Set destinasi instalasi ke: C:\Java\jdk-11\.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/588/1*zSAlXjUhdxufQ6fOhVg38A.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/781/1*A20tv_pqt5De0_8BvAeBjA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/781/1*xdsrXUFdPP3Vd08iU1NCbw.png" /></figure><h4>Ekstraksi dan Penyiapan Hadoop, Spark, dan Winutils</h4><ul><li>Ekstrak Hadoop, Spark, dan Winutils ke direktori:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*7ZhF5TrBxm4YFfF1kizICw.png" /></figure><ul><li><strong>Hadoop</strong>: C:\hadoop-3.3.6\</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*Hd6JXQJaQyqkdCHRBxoDOw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*db4fd8Qj5S0Xe4h0F_ud0A.png" /></figure><ul><li>Wizard.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/683/1*zBPnt3HUDv-vaBFSMPXjKg.png" /></figure><ul><li><strong>Spark</strong>: C:\spark-3.5.2-bin-hadoop3\</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*1LmcaQg3RB7NYT0i8n5_Dw.png" /></figure><ul><li>Wizard.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/683/1*_JpBRmd0obUEKx_oJPfCEQ.png" /></figure><ul><li><strong>Winutils</strong>: salin folder bin dari Winutils ke direktori Hadoop: C:\hadoop-3.3.6\.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*6ZKejPUZJ9dhpvORJnAbzw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/683/1*NBMuPlAra1Aahq0QdaK5Wg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/598/1*HxZD5JyzK0rQk6uyAQQNOg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/702/1*8E2-w9nNtQAEpuSd8ouwLA.png" /></figure><h4>Konfigurasi Environment Variables</h4><ul><li>Buka <strong>System Properties</strong> > <strong>Advanced</strong> > <strong>Environment Variables</strong>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*38MncuI47NsC679CVt7HpA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/644/1*FdcMXFrG0PFHHizEFltVIQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*F9uULwWq1BemywVSSRDl0w.png" /></figure><ul><li>Tambahkan variabel baru berikut:</li></ul><pre>HADOOP_CONF_FILE = C:\hadoop-3.3.6\etc\hadoop<br />HADOOP_HOME = C:\hadoop-3.3.6<br />JAVA_HOME = C:\Java\jdk-11<br />SPARK_HOME = C:\spark-3.5.2-bin-hadoop3</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*7x8MUhgJ1bJ3QKxlWAF8SQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*IaanF18RcxhF5QtDlcDCsw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*Ph8Js-hZuN2BlMoMTyOcnA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*ITPHbw0Mw9-wYdl_3Oahvw.png" /></figure><ul><li>Edit Path dan tambahkan:</li></ul><pre>%JAVA_HOME%\bin<br />%HADOOP_HOME%\bin<br />%HADOOP_HOME%\sbin<br />%SPARK_HOME%\bin</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*b_v3q_WJorkzfg-E-tYLHQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/824/1*SpJVTo0tH_qNS5sQnQSZOg.png" /></figure><h4>Konfigurasi Hadoop</h4><ul><li>Edit file konfigurasi di <strong>C:\hadoop-3.3.6\etc\hadoop</strong>:</li><li><strong>core-site.xml:</strong></li></ul><pre><configuration><br />  <property><br />    <name>fs.default.name</name><br />    <value>hdfs://0.0.0.0:9000</value><br />  </property><br /></configuration></pre><ul><li><strong>hdfs-site.xml:</strong></li></ul><pre><configuration><br />  <property><br />    <name>dfs.replication</name><br />    <value>1</value><br />  </property><br />  <property><br />    <name>dfs.namenode.name.dir</name><br />    <value>/hadoop-3.3.6/data/namenode</value><br />  </property><br />  <property><br />    <name>dfs.datanode.data.dir</name><br />    <value>/hadoop-3.3.6/data/datanode</value><br />  </property><br /></configuration></pre><ul><li><strong>mapred-site.xml</strong>:</li></ul><pre><configuration><br />  <property><br />    <name>mapreduce.framework.name</name><br />    <value>yarn</value><br />  </property><br />  <property><br />    <name>mapreduce.application.classpath</name><br />    <value>%HADOOP_HOME%/share/hadoop/mapreduce/*</value><br />  </property><br /></configuration></pre><ul><li><strong>yarn-site.xml</strong>:</li></ul><pre><configuration><br />  <property><br />    <name>yarn.nodemanager.aux-services</name><br />    <value>mapreduce_shuffle</value><br />  </property><br /></configuration></pre><h4>Pengujian Hadoop dan Spark</h4><ul><li>Untuk memverifikasi instalasi, jalankan perintah berikut di Command Prompt:</li></ul><pre>java -version<br />hadoop version<br />spark-shell</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*NrEmq3eh_zO-2a9PnIJLfA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*hBtOrA32iKtxNDJsl7adCg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*djMwWL1Xz24U5OlbWtLfIw.png" /></figure><ul><li>Ctrl+c untuk keluar dari spark-shell.</li><li>Format namenode:</li></ul><pre>hdfs namenode -format</pre><ul><li>Pastikan di direktori <strong>C:\hadoop-3.3.6</strong> muncul folder data.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*oTAE4gzNS2Lr58Bz6GpBZA.png" /></figure><ul><li>Mulai Hadoop dan YARN:</li></ul><pre>start-all.cmd</pre><ul><li>Jika command promt yarn muncul:</li></ul><pre>error Couldn't find a package.json file in "C:\\Users\\User"<br />info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.</pre><ul><li>Maka edit file <strong>start-yarn.cmd</strong> dan <strong>stop-yarn.cmd</strong> di direktori <strong>C:\hadoop-3.3.6\sbin</strong> pada bagian paling bawah sebelum enlocal</li><li><strong>start-yarn.cmd:</strong></li></ul><pre>@rem start resourceManager<br />start "Apache Hadoop Distribution" %HADOOP_HOME%\bin\yarn resourcemanager<br />@rem start nodeManager<br />start "Apache Hadoop Distribution" %HADOOP_HOME%\bin\yarn nodemanager<br />@rem start proxyserver<br />@rem start "Apache Hadoop Distribution" yarn proxyserver</pre><ul><li><strong>stop-yarn.cmd:</strong></li></ul><pre>@rem stop resourceManager<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - %HADOOP_HOME%\bin\yarn   resourcemanager"<br />@rem stop nodeManager<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - %HADOOP_HOME%\bin\yarn   nodemanager"<br />@rem stop proxy server<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - yarn   proxyserver"</pre><ul><li>start-yarn tunggu hingga selesai lalu cek <a href="http://localhost:8088">http://localhost:8088</a> pastikan pada bagian active nodes muncul angka 1 setalah yarn selesai di jalankan.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*bqed42btMAnT1g-8jEiZ1Q.png" /></figure><ul><li>Jika tidak, maka matikan yarn terlebih dahulu dengan stop-yarn lalu ubah file <strong>yarn-site.xml</strong> di direktori <strong>C:\hadoop-3.3.6\etc\hadoop</strong> tambahkan property di dalam configurasi:</li></ul><pre><property><br />  <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name><br />  <value>95</value><br /></property></pre><ul><li>Cek UI web di Hadoop Namenode <a href="http://localhost:9870">http://localhost:9870</a>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*oNPhWq0WtUuIyy6AEEFRfQ.png" /></figure><ul><li>Ketik JPS untuk memastikan berjalan semua.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*5i3bTl2Qap_vOen_x9Ygjg.png" /></figure><h4>Pengujian HDFS</h4><ul><li>Buat direktori dan upload file ke HDFS:</li></ul><pre>hdfs dfs -mkdir /user<br />hdfs dfs -mkdir /user/figuran04<br />echo "hello" > input.txt<br />hdfs dfs -put input.txt /user/figuran04<br />hdfs dfs -ls /user/figuran04/</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*Td4WGoqR4JCTwuYu7VZfrw.png" /></figure><ul><li>Operasi File di HDFS:</li></ul><pre>hdfs dfs -cat /user/figuran04/input.txt<br />hdfs dfs -cp /user/figuran04/input.txt /user/figuran04/input_copy.txt<br />hdfs dfs -ls /user/figuran04/<br />hdfs dfs -rm /user/figuran04/input_copy.txt<br />hdfs dfs -ls /user/figuran04/</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*Cwijclefi4V38LzaAKys0g.png" /></figure><ul><li>Menganalisis Struktur Penyimpanan di HDFS:</li></ul><pre>hdfs dfsadmin -report<br />hdfs fsck / -files -blocks -locations</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*MstW9wHST7goqQcABDM7Uw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*SYbEO9QnnZ-H4KSqADWtRQ.png" /></figure><ul><li>Laporan hasil dari hdfs dfsadmin -report :</li></ul><h4>1. Kapasitas Sistem dan Penggunaan DFS</h4><ul><li><strong>Kapasitas Terpasang (Configured Capacity)</strong>: 103.37 GB</li></ul><p>Ini adalah total kapasitas penyimpanan fisik yang dikonfigurasi di HDFS, yang mencerminkan ukuran keseluruhan disk yang tersedia untuk digunakan oleh sistem.</p><ul><li><strong>Kapasitas Saat Ini (Present Capacity)</strong>: 7.91 GB</li></ul><p>Kapasitas saat ini menunjukkan ruang penyimpanan yang tersedia untuk DFS setelah dikurangi penggunaan non-DFS, seperti file sistem operasi dan file lainnya yang tidak disimpan di HDFS.</p><ul><li><strong>DFS Used</strong>: 343 B</li></ul><p>Kapasitas yang digunakan oleh DFS sangat kecil (343 B), menunjukkan bahwa hampir tidak ada data besar yang disimpan di HDFS pada saat laporan dibuat.</p><ul><li><strong>DFS Remaining</strong>: 7.91 GB</li></ul><p>Ini adalah kapasitas sisa yang dapat digunakan oleh HDFS untuk menyimpan data tambahan. HDFS memiliki ruang yang cukup besar (7.91 GB) yang tersedia untuk penyimpanan lebih lanjut.</p><h4>2. Persentase Penggunaan DFS</h4><ul><li><strong>DFS Used%</strong>: 0.00%</li></ul><p>Penggunaan DFS saat ini hampir nol persen, menandakan bahwa HDFS tidak banyak digunakan untuk menyimpan data pada tahap ini. Hal ini mungkin terjadi jika baru sedikit data yang dimasukkan ke dalam sistem, seperti hanya satu file atau beberapa file kecil.</p><p><strong>DFS Remaining%</strong>: 8.22%</p><p>Persentase kapasitas yang tersisa cukup signifikan, dengan 8.22% dari total kapasitas tersedia untuk penyimpanan lebih lanjut.</p><h4>3. Status Replikasi Blok</h4><ul><li><strong>Under-replicated blocks</strong>: 0</li></ul><p>Tidak ada blok yang kurang direplikasi. Ini menunjukkan bahwa semua blok disalin sesuai dengan faktor replikasi yang diatur (1 dalam konfigurasi ini).</p><ul><li><strong>Blocks with corrupt replicas</strong>: 0</li></ul><p>Tidak ada blok dengan replika yang rusak, menandakan bahwa semua replika yang disimpan di datanode berada dalam kondisi yang baik dan tidak ada kerusakan pada blok data.</p><ul><li><strong>Missing blocks</strong>: 0</li></ul><p>Tidak ada blok yang hilang dari sistem, sehingga menunjukkan integritas data yang baik.</p><ul><li><strong>Low redundancy blocks with highest priority to recover</strong>: 0</li></ul><p>Tidak ada blok dengan redundansi rendah yang perlu dipulihkan segera, artinya semua blok direplikasi dengan benar dan tidak memerlukan tindakan segera untuk pemulihan.</p><h4>4. Erasure Coded Block Groups</h4><p>Pada laporan ini, tidak ada erasure coded block groups yang digunakan, sehingga semua blok disimpan dan direplikasi dengan metode replikasi standar, tanpa menggunakan kode penghapusan untuk efisiensi penyimpanan.</p><h4>5. Informasi Datanode</h4><ul><li><strong>Nama Datanode</strong>: 192.168.42.16:9866 (DESKTOP-C7L4FR9)</li></ul><p>Sistem HDFS saat ini hanya memiliki satu datanode yang aktif dengan status dekomisionisasi: <strong>Normal</strong>.</p><ul><li><strong>Kapasitas Terpasang pada Datanode</strong>: 103.37 GB</li></ul><p>Sama seperti kapasitas sistem secara keseluruhan, karena hanya ada satu datanode.</p><ul><li><strong>DFS Used pada Datanode</strong>: 343 B</li></ul><p>Menunjukkan bahwa hanya sedikit data yang tersimpan pada datanode ini, sesuai dengan keseluruhan penggunaan DFS.</p><ul><li><strong>Non-DFS Used pada Datanode</strong>: 88.36 GB</li></ul><p>Sebagian besar ruang penyimpanan pada datanode digunakan oleh file dan data di luar HDFS, seperti sistem operasi atau file lokal lainnya.</p><ul><li><strong>DFS Remaining pada Datanode</strong>: 7.91 GB</li></ul><p>Kapasitas penyimpanan yang tersisa untuk DFS sama seperti kapasitas yang tersedia di keseluruhan sistem.</p><h4>6. Blok dan Transaksi</h4><ul><li><strong>Xceivers</strong>: 0</li></ul><p>Ini menunjukkan bahwa tidak ada transaksi yang sedang berlangsung atau tidak ada komunikasi aktif antara client dan datanode pada saat laporan dibuat.</p><ul><li><strong>Num of Blocks</strong>: 1</li></ul><p>Hanya ada satu blok data yang tersimpan di HDFS, yang mengindikasikan bahwa volume data yang tersimpan di HDFS sangat kecil.</p><h4>Integrasi Spark dengan Hadoop</h4><ul><li>Jalankan Spark shell dan uji akses HDFS:</li></ul><pre>val data = sc.textFile("hdfs://localhost:9000/user/figuran04/input.txt")<br />val line = data.first()<br />val charCount = line.length<br />println(s"The number of characters in the line is: $charCount")</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*hBtOrA32iKtxNDJsl7adCg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*jvd1f9zXsI6_vvr3beT4_g.png" /></figure><h3>Contoh Kasus dan Penyelesaiannya</h3><p><strong><em>Hanya jps yang muncul saat menjalankan jps setelah start-dfs?</em></strong></p><ul><li>Cek apakah winutils.exe sudah ada di C:\Hadoop\hadoop-3.4.0\bin. Jika tidak, pastikan telah mengunduh dan menyalinnya ke direktori yang benar.</li></ul><p><strong><em>Hanya jps dan salah satu antara namenode dan datanode yang muncul?</em></strong></p><ul><li>Tutup kedua cmd tersebut lalu masuk ke C:\Hadoop\hadoop-3.4.0\data hapus yang ada di dalam datanode dan namenode lalu format ulang hdfs namenode -format dan jalankan kembali start-dfs.sh.</li></ul><p><strong><em>Gagal memformat HDFS dengan pesan kesalahan “Could not locate executable winutils.exe”?</em></strong></p><ul><li>Pastikan HADOOP_HOME dan Path sudah terkonfigurasi dengan benar. Jika masalah berlanjut, pastikan file winutils.exe ada di direktori bin.</li></ul><p><strong><em>Spark tidak bisa membaca file dari HDFS?</em></strong></p><ul><li>Periksa kembali alamat file di HDFS dan pastikan Hadoop sedang berjalan dengan benar.</li></ul><p><strong><em>Mengapa </em></strong><strong><em>nodemanager dan </em></strong><strong><em>resourcemanager tidak dapat berjalan ketika command </em></strong><strong><em>start-all atau </em></strong><strong><em>start-yarn ?</em></strong></p><ul><li>Patikan versi Java atau JDK ke 11 download <a href="https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11024/jdk-11.0.24_windows-x64_bin.exe">di sini</a>.</li><li>Ganti JDK nodemanager dan resourcemanager masih belum berjalan? silahkan masuk ke C:\Hadoop\hadoop-3.4.0\sbin lalu ubah start-yarn.cmd dan stop-yarn.cmd pada bagian paling bawah sebelum <strong>endlocal</strong> menjadi seperti ini:</li><li><strong>start-yarn.cmd:</strong></li></ul><pre>@rem start resourceManager<br />start "Apache Hadoop Distribution" %HADOOP_HOME%\bin\yarn resourcemanager<br />@rem start nodeManager<br />start "Apache Hadoop Distribution" %HADOOP_HOME%\bin\yarn nodemanager<br />@rem start proxyserver<br />@rem start "Apache Hadoop Distribution" yarn proxyserver</pre><ul><li><strong>stop-yarn.cmd:</strong></li></ul><pre>@rem stop resourceManager<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - %HADOOP_HOME%\bin\yarn   resourcemanager"<br />@rem stop nodeManager<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - %HADOOP_HOME%\bin\yarn   nodemanager"<br />@rem stop proxy server<br />Taskkill /FI "WINDOWTITLE eq Apache Hadoop Distribution - yarn   proxyserver"</pre><p><strong><em>Bagian active nodes muncul angka 0 bukan 1 pada http:localhost:8088 setalah yarn selesai di jalankan.</em></strong></p><ul><li>Matikan yarn terlebih dahulu dengan stop-yarn lalu ubah file <strong>yarn-site.xml</strong> di direktori <strong>C:\hadoop-3.3.6\etc\hadoop</strong> tambahkan property di dalam configurasi:</li></ul><pre><property><br />  <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name><br />  <value>95</value><br /></property></pre><h3>Kesimpulan</h3><p>Dengan mengikuti langkah-langkah di atas, Anda telah berhasil menginstal dan mengonfigurasi Hadoop serta Spark di Windows. Jika terjadi masalah, periksa konfigurasi file dan environment variables Anda. Eksperimenlah dengan berbagai pengaturan dan jangan ragu untuk menyesuaikan setup ini sesuai kebutuhan. Jika ada pertanyaan atau masalah lain, silakan tinggalkan komentar!</p><h3>Referensi</h3><ul><li><a href="https://drive.google.com/file/d/1yHm-l7yONO0BuWynX0V15IGlI1erPYDw/view?usp=sharing">Hands_On_Pertemuan_2.ipynb</a></li><li><a href="https://medium.com/@ikhsan.budi.wicaksono/how-to-install-81b97de7a9b9">How To Install</a></li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f7f3582def93" width="1" />
<!--END_SECTION:medium-->