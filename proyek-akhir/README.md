<!--START_SECTION:medium-->
[Baca di Medium](https://medium.com/@dikaelsaputra/memprediksi-keberhasilan-tmdb-konten-tv-7b6826951b50?source=rss-272e0aace4a6------2)

<h3>Memprediksi Keberhasilan TMDB & Konten TV: Menggunakan Logistic Regression dan Linear Regression</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/793/0*Qm_w9_yZFc8YE4H0" /></figure><p>Pada kesempatan kali ini, kita akan belajar cara menggunakan model <strong>Logistic Regression</strong> dan <strong>Linear Regression</strong> untuk menganalisis dan memprediksi keberhasilan The Movie Database dan konten TV. Dalam analisis ini, kita akan menggunakan dataset yang berisi informasi mengenai film dan acara TV, seperti <em>popularitas</em>, <em>vote count</em>, <em>episode runtime</em>, dan lainnya. Kita akan membahas kedua teknik regresi ini untuk menyelesaikan masalah klasifikasi dan prediksi numerik.</p><p>Sebelum kita memulai, penting untuk memahami bahwa <strong>Logistic Regression</strong> digunakan untuk klasifikasi biner (misalnya, prediksi apakah sebuah acara TV akan sukses atau tidak), sementara <strong>Linear Regression</strong> digunakan untuk memprediksi nilai numerik kontinu (misalnya, prediksi rating atau popularitas acara TV).</p><p>Mari kita bahas lebih lanjut bagaimana kedua teknik ini diterapkan pada dataset konten TV.</p><h3>Persiapan Data dan Pembersihan</h3><p>Sebelum kita membangun model, data harus dipersiapkan dan dibersihkan terlebih dahulu. Hal ini meliputi pengecekan data yang hilang, pengisian data yang hilang, serta penanganan duplikasi dan outlier.</p><ul><li><strong>Memuat dan Memahami Dataset: </strong>Langkah pertama adalah memuat data yang akan kita analisis. Dataset ini berada dalam format CSV, dan kita memuatnya ke dalam sebuah <em>DataFrame</em> menggunakan pandas. Dengan melihat beberapa baris pertama dari data, kita bisa mendapatkan gambaran umum mengenai struktur dan isi data.</li></ul><pre>import pandas as pd<br /><br />df = pd.read_csv("/content/drive/MyDrive/TMDB_tv_dataset_v3.csv")<br />print(df.head())<br />print(df.info())</pre><p><strong>Output</strong></p><pre>      id              name  number_of_seasons  number_of_episodes  \<br />0   1399   Game of Thrones                  8                  73   <br />1  71446       Money Heist                  3                  41   <br />2  66732   Stranger Things                  4                  34   <br />3   1402  The Walking Dead                 11                 177   <br />4  63174           Lucifer                  6                  93   <br /><br />  original_language  vote_count  vote_average  \<br />0                en       21857         8.442   <br />1                es       17836         8.257   <br />2                en       16161         8.624   <br />3                en       15432         8.121   <br />4                en       13870         8.486   <br /><br />                                            overview  adult  \<br />0  Seven noble families fight for control of the ...  False   <br />1  To carry out the biggest heist in history, a m...  False   <br />2  When a young boy vanishes, a small town uncove...  False   <br />3  Sheriff's deputy Rick Grimes awakens from a co...  False   <br />4  Bored and unhappy as the Lord of Hell, Lucifer...  False   <br /><br />                      backdrop_path  ...                           tagline  \<br />0   /2OMB0ynKlyIenMJWI2Dy9IWT4c.jpg  ...                  Winter Is Coming   <br />1  /gFZriCkpJYsApPZEF3jhxL4yLzG.jpg  ...              The perfect robbery.   <br />2  /2MaumbgBlW1NoPo3ZJO38A6v7OS.jpg  ...     Every ending has a beginning.   <br />3  /x4salpjB11umlUOltfNvSSrjSXm.jpg  ...  Fight the dead. Fear the living.   <br />4  /aDBRtunw49UF4XmqfyNuD9nlYIu.jpg  ...              It's good to be bad.   <br /><br />                                        genres                 created_by  \<br />0  Sci-Fi & Fantasy, Drama, Action & Adventure  David Benioff, D.B. Weiss   <br />1                                 Crime, Drama                  Álex Pina   <br />2             Drama, Sci-Fi & Fantasy, Mystery   Matt Duffer, Ross Duffer   <br />3  Action & Adventure, Drama, Sci-Fi & Fantasy             Frank Darabont   <br />4                      Crime, Sci-Fi & Fantasy                Tom Kapinos   <br /><br />   languages           networks  origin_country spoken_languages  \<br />0         en                HBO              US          English   <br />1         es  Netflix, Antena 3              ES          Español   <br />2         en            Netflix              US          English   <br />3         en                AMC              US          English   <br />4         en       FOX, Netflix              US          English   <br /><br />                                production_companies  \<br />0  Revolution Sun Studios, Television 360, Genera...   <br />1                                    Vancouver Media   <br />2  21 Laps Entertainment, Monkey Massacre Product...   <br />3  AMC Studios, Circle of Confusion, Valhalla Mot...   <br />4  Warner Bros. Television, DC Entertainment, Jer...   <br /><br />                       production_countries episode_run_time  <br />0  United Kingdom, United States of America                0  <br />1                                     Spain               70  <br />2                  United States of America                0  <br />3                  United States of America               42  <br />4                  United States of America               45  <br /><br />[5 rows x 29 columns]<br /><class 'pandas.core.frame.DataFrame'><br />RangeIndex: 168639 entries, 0 to 168638<br />Data columns (total 29 columns):<br /> #   Column                Non-Null Count   Dtype  <br />---  ------                --------------   -----  <br /> 0   id                    168639 non-null  int64  <br /> 1   name                  168634 non-null  object <br /> 2   number_of_seasons     168639 non-null  int64  <br /> 3   number_of_episodes    168639 non-null  int64  <br /> 4   original_language     168639 non-null  object <br /> 5   vote_count            168639 non-null  int64  <br /> 6   vote_average          168639 non-null  float64<br /> 7   overview              93333 non-null   object <br /> 8   adult                 168639 non-null  bool   <br /> 9   backdrop_path         77780 non-null   object <br /> 10  first_air_date        136903 non-null  object <br /> 11  last_air_date         138735 non-null  object <br /> 12  homepage              50998 non-null   object <br /> 13  in_production         168639 non-null  bool   <br /> 14  original_name         168634 non-null  object <br /> 15  popularity            168639 non-null  float64<br /> 16  poster_path           108737 non-null  object <br /> 17  type                  168639 non-null  object <br /> 18  status                168639 non-null  object <br /> 19  tagline               5330 non-null    object <br /> 20  genres                99713 non-null   object <br /> 21  created_by            36496 non-null   object <br /> 22  languages             110050 non-null  object <br /> 23  networks              97589 non-null   object <br /> 24  origin_country        137609 non-null  object <br /> 25  spoken_languages      109280 non-null  object <br /> 26  production_companies  59342 non-null   object <br /> 27  production_countries  77511 non-null   object <br /> 28  episode_run_time      168639 non-null  int64  <br />dtypes: bool(2), float64(2), int64(5), object(20)<br />memory usage: 35.1+ MB<br />None</pre><ul><li><strong>Pengecekan dan Pengisian Data yang Hilang</strong>: Kita akan memeriksa nilai yang hilang pada dataset dan mengisi beberapa kolom dengan nilai default atau metode yang sesuai.</li></ul><pre># Identifikasi nilai yang hilang<br />missing_data = df.isnull().sum()<br />print("Nilai yang hilang per kolom:\n", missing_data)<br /><br /># Menghapus kolom dengan lebih dari 50% data hilang<br />threshold = 0.5 * len(df)<br />df.dropna(thresh=threshold, axis=1, inplace=True)<br /><br /># Menghapus baris yang kolom 'name' hilang<br />df.dropna(subset=['name'], inplace=True)<br /><br /># Mengisi nilai hilang pada kolom 'genres' dengan 'Unknown' dan mengubahnya menjadi lowercase<br />df['genres'] = df['genres'].fillna('Unknown').str.lower()</pre><p><strong>Menghapus kolom dengan banyak nilai hilang</strong> kolom yang memiliki lebih dari 50% data hilang akan dihapus. <strong>Menghapus baris dengan nilai hilang pada kolom penting </strong>dalam hal ini, kolom ‘name’ harus ada untuk setiap baris, jadi jika ada nilai hilang di kolom ini, kita akan menghapus barisnya. <strong>Mengisi nilai hilang pada kolom ‘genres’ dengan ‘Unknown’ </strong>kolom ini akan diisi dengan ‘Unknown’ dan kata-kata dalam kolom ini akan diubah menjadi huruf kecil.</p><p><strong>Before and after</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/381/1*1Tv13-CFvF6b6xHi_NIYKA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/374/1*iXcClz2QfivxIg63YWxAqg.png" /></figure><p>Dataset ini memiliki kolom negara asal dan bahasa asli yang terkadang berisi nilai hilang. Kita dapat mengisinya berdasarkan informasi negara dengan menggunakan pemetaan yang telah ditentukan. Misalnya, jika negara asal adalah AS (US), maka bahasa asli mungkin ‘English’. Fungsi standardize_and_fill_na() digunakan untuk mengisi kolom negara dan bahasa dengan nilai yang sesuai berdasarkan pemetaan yang telah kita buat.</p><pre>country_map = {<br />    'US': 'English', 'ES': 'Español', 'JP': '日本語', 'KR': '한국어',<br />    'FR': 'Français', 'DE': 'Deutsch', 'IT': 'Italiano', 'IN': 'हिन्दी',<br />    'GB': 'English', 'CA': 'English', 'AU': 'English', 'BR': 'Português',<br />    'MX': 'Español', 'RU': 'Русский'<br />}<br /><br /># Standarisasi dan isi nilai hilang berdasarkan negara<br />def standardize_and_fill_na(col, ref_col):<br />    col = col.astype(str).str.upper().fillna('UNKNOWN')<br />    ref_col = ref_col.astype(str).str.upper().fillna('UNKNOWN')<br />    return col.map(country_map).where(ref_col == 'US', col).where(ref_col == 'ES', col)<br /><br />df['origin_country'] = standardize_and_fill_na(df['origin_country'], df['original_language'])<br />df['original_language'] = standardize_and_fill_na(df['original_language'], df['origin_country'])<br />df['languages'] = standardize_and_fill_na(df['languages'], df['origin_country'])<br /><br />def fill_spoken_languages(row):<br />    if row['origin_country'] in ['US', 'EN', 'CA', 'AU', 'GB']:<br />        return 'English'<br />    elif row['origin_country'] in ['ES', 'MX']:<br />        return 'Español'<br />    elif row['origin_country'] == 'JP':<br />        return '日本語'<br />    elif row['origin_country'] == 'KR':<br />        return '한국어'<br />    elif row['origin_country'] == 'FR':<br />        return 'Français'<br />    elif row['origin_country'] == 'DE':<br />        return 'Deutsch'<br />    elif row['origin_country'] == 'IT':<br />        return 'Italiano'<br />    elif row['origin_country'] == 'IN':<br />        return 'हिन्दी'<br />    elif row['origin_country'] == 'BR':<br />        return 'Português'<br />    elif row['origin_country'] == 'RU':<br />        return 'Русский'<br />    else:<br />        return 'Unknown'<br /><br />df['spoken_languages'] = df.apply(fill_spoken_languages, axis=1)<br /><br /># Drop Columns and Rows Based on Updated Threshold<br />threshold = 0.7 * len(df)<br />df.dropna(thresh=threshold, axis=1, inplace=True)<br />df.dropna(subset=['first_air_date', 'last_air_date'], inplace=True)</pre><p><strong>Finally</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/226/1*P1vocO7UREpjuuB01vskqA.png" /></figure><p><strong>Penanganan Outlier</strong>: Kita juga akan menangani outlier yang bisa mempengaruhi hasil model kita. Outlier adalah nilai yang jauh dari nilai mayoritas dalam data dan dapat mempengaruhi hasil analisis. Dalam kasus ini, kita mendeteksi outlier pada kolom numerik seperti jumlah episode, jumlah suara, dan popularitas. Kita menggunakan Interquartile Range (IQR) untuk mendeteksi dan menangani outlier.</p><pre># Deteksi outlier menggunakan IQR<br />numerical_columns = [<br />    "number_of_seasons", "number_of_episodes", "vote_count", "vote_average", "popularity", "episode_run_time"<br />]<br />Q1 = df[numerical_columns].quantile(0.25)<br />Q3 = df[numerical_columns].quantile(0.75)<br />IQR = Q3 - Q1<br /><br />lower_bound = Q1 - 1.5 * IQR<br />upper_bound = Q3 + 1.5 * IQR<br /><br /># Tandai data sebagai outlier<br />outliers = ((df[numerical_columns] < lower_bound) | (df[numerical_columns] > upper_bound))<br />outliers_summary = outliers.sum()<br />print("Jumlah outlier per kolom:\n", outliers_summary)</pre><p><strong>Output</strong></p><pre>Jumlah outlier per kolom:<br />number_of_seasons     30396<br />number_of_episodes    13092<br />vote_count            16332<br />vote_average              0<br />popularity            18229<br />episode_run_time       2223<br />dtype: int64</pre><p>Setelah mendeteksi outlier, kita mengganti nilai outlier dengan median kolom terkait.</p><pre>cols_to_impute = ["number_of_seasons", "number_of_episodes"]<br />for col in cols_to_impute:<br />    median = df[col].median()<br />    df[col] = df[col].mask(outliers[col], median)<br /><br /># Log Transformation<br />df["popularity"] = np.log1p(df["popularity"])<br />df["vote_count"] = np.log1p(df["vote_count"])<br /><br /># Visualizations<br />fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)<br /><br /># Histogram for "popularity"<br />sns.histplot(df["popularity"], kde=True, color="blue", bins=20, ax=axes[0])<br />axes[0].set_title("Distribusi Popularitas Setelah Transformasi Log")<br />axes[0].set_xlabel("Popularitas (Log Transformed)")<br />axes[0].set_ylabel("Frekuensi")<br /><br /># Histogram for "vote_count"<br />sns.histplot(df["vote_count"], kde=True, color="green", bins=20, ax=axes[1])<br />axes[1].set_title("Distribusi Vote Count Setelah Transformasi Log")<br />axes[1].set_xlabel("Vote Count (Log Transformed)")<br />axes[1].set_ylabel("Frekuensi")<br /><br />plt.tight_layout()<br />plt.show()<br /><br /># Filter "episode_run_time" and visualize<br />df = df[(df["episode_run_time"] >= 1) & (df["episode_run_time"] <= 200)]<br />print(df[["episode_run_time"]])<br /><br />sns.histplot(df["episode_run_time"], bins=30, kde=True, color="green")<br />plt.title("Distribusi episode_run_time (Setelah Filter)")<br />plt.xlabel("Episode Run Time")<br />plt.ylabel("Frekuensi")<br />plt.show()</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*IV2mdcIr3CnL8qWbA_WOyA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/1*9dw3kJ7mQw_pYcRuDHRWlg.png" /></figure><p>Visualisasi ini membantu kita memahami distribusi data dan bagaimana transformasi logaritma mempengaruhi data.</p><pre># Statistics<br />print(df.describe())</pre><p><strong>Output</strong></p><pre>                  id  number_of_seasons  number_of_episodes    vote_count  \<br />count   71723.000000            71723.0        71723.000000  71723.000000   <br />mean    87652.868745                1.0           14.255720      1.076015   <br />std     63203.546338                0.0           12.351731      1.439634   <br />min         1.000000                1.0            0.000000      0.000000   <br />25%     35135.000000                1.0            6.000000      0.000000   <br />50%     79247.000000                1.0           10.000000      0.693147   <br />75%    122489.500000                1.0           20.000000      1.609438   <br />max    251079.000000                1.0           56.000000      9.789030   <br /><br />       vote_average    popularity  episode_run_time  <br />count  71723.000000  71723.000000      71723.000000  <br />mean       3.856682      1.434447         42.772751  <br />std        3.659558      1.052542         24.595488  <br />min        0.000000      0.470004          1.000000  <br />25%        0.000000      0.629142         25.000000  <br />50%        5.000000      1.085189         43.000000  <br />75%        7.200000      1.892886         55.000000  <br />max       10.000000      8.218250        200.000000  </pre><h3>Logistic Regression untuk Klasifikasi Keberhasilan Konten TV</h3><p>Setelah data siap, kita bisa melanjutkan dengan model <strong>Logistic Regression</strong> untuk memprediksi apakah sebuah acara TV akan sukses atau tidak. Keberhasilan diukur dengan berbagai fitur seperti <em>vote_average</em>, <em>popularity</em>, dan <em>vote_count</em>. Logistic Regression bekerja dengan baik untuk masalah klasifikasi biner, dan kita dapat menggunakan metrik seperti <strong>AUC (Area Under Curve)</strong> untuk mengukur kinerjanya.</p><p><strong>Langkah-langkah:</strong></p><ul><li><strong>Persiapkan Data</strong>: Pilih fitur yang relevan untuk klasifikasi.</li><li><strong>Latih Model</strong>: Gunakan Logistic Regression untuk memprediksi apakah acara akan sukses atau tidak.</li><li><strong>Evaluasi Model</strong>: Gunakan metrik AUC untuk mengevaluasi kinerja model.</li></ul><h4><strong>Persiapkan Data</strong></h4><pre>from pyspark.sql import SparkSession<br />from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler<br />from pyspark.ml.classification import LogisticRegression<br />from pyspark.ml.evaluation import BinaryClassificationEvaluator<br />from pyspark.sql.functions import when, col, udf<br />from pyspark.sql.types import IntegerType<br />from pyspark.ml.linalg import Vectors, VectorUDT<br />from pyspark.ml.tuning import CrossValidator, ParamGridBuilder<br />from pyspark.ml import Pipeline<br />import pandas as pd<br />import numpy as np<br />from imblearn.over_sampling import SMOTE<br />from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score<br />import seaborn as sns<br />import matplotlib.pyplot as plt<br /><br />spark = SparkSession.builder.appName("MLlib Tasks").getOrCreate()<br /><br /># Mengonversi DataFrame ke Spark DataFrame<br />spark_df = spark.createDataFrame(df)<br /><br /># Menghitung median dari kolom 'popularity'<br />median_popularity = spark_df.approxQuantile("popularity", [0.5], 0)[0]<br /><br /># Membuat kolom target<br />spark_df = spark_df.withColumn(<br />    "successful_content",<br />    when((spark_df.popularity > median_popularity) & (spark_df.vote_average >= 7) & (spark_df.vote_count > 0.1), 1).otherwise(0)<br />)<br /><br /># Mengubah kolom kategori menjadi indeks numerik<br />indexer = StringIndexer(inputCols=["original_language", "type"], outputCols=["original_language_index", "type_index"])<br />indexed_df = indexer.fit(spark_df).transform(spark_df)<br /><br /># Membuat kolom fitur<br />feature_columns = [<br />    "number_of_episodes", "vote_count",<br />    "vote_average", "popularity", "type_index"<br />]<br />assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")<br />assembled_df = assembler.transform(indexed_df)<br /><br /># Standarisasi fitur<br />scaler = StandardScaler(inputCol="features", outputCol="scaled_features")<br />scaled_df = scaler.fit(assembled_df).transform(assembled_df)<br /># Konversi ke Pandas DataFrame untuk SMOTE<br />pandas_df = scaled_df.select("successful_content", "scaled_features").toPandas()<br />X = pandas_df["scaled_features"].apply(lambda x: x.toArray()).tolist()<br />y = pandas_df["successful_content"].tolist()<br /><br /># Oversampling dengan SMOTE<br />smote = SMOTE(random_state=42)<br />X_resampled, y_resampled = smote.fit_resample(X, y)<br /><br /># Konversi kembali ke Spark DataFrame<br />to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())<br />balanced_df = spark.createDataFrame(pd.DataFrame({"scaled_features": X_resampled, "successful_content": y_resampled}))<br />balanced_df = balanced_df.withColumn("scaled_features", to_vector("scaled_features"))</pre><h4><strong>Latih Model</strong></h4><pre># Membagi data menjadi train dan test<br />train_df, test_df = balanced_df.randomSplit([0.8, 0.2], seed=42)<br /><br /># Logistic Regression<br />lr = LogisticRegression(featuresCol='scaled_features', labelCol='successful_content', maxIter=100, regParam=0.01, elasticNetParam=0.8)<br />logisticRegressionModel = lr.fit(train_df)<br /><br /># Evaluasi model<br />predictions = logisticRegressionModel.transform(test_df)<br />evaluator = BinaryClassificationEvaluator(labelCol="successful_content", metricName="areaUnderROC")<br />roc_auc = evaluator.evaluate(predictions)<br /><br /># Menampilkan hasil<br />print(f"ROC AUC: {roc_auc}")<br />for name, coef in zip(feature_columns, logisticRegressionModel.coefficients):<br />    print(f"Feature: {name}, Coefficient: {coef}")<br />print(f'Intercept: {logisticRegressionModel.intercept}')<br /><br /># Pipeline dan parameter grid<br />pipeline = Pipeline(stages=[lr])<br />paramGrid = ParamGridBuilder() \<br />    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \<br />    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \<br />    .build()<br /><br /># Cross-Validation<br />crossval = CrossValidator(<br />    estimator=pipeline,<br />    estimatorParamMaps=paramGrid,<br />    evaluator=evaluator,<br />    numFolds=5,<br />    parallelism=2<br />)<br /><br /># Melatih model dengan Cross-Validation<br />cv_model = crossval.fit(train_df)<br /><br /># Evaluasi model terbaik<br />bestLogisticRegressionModel = cv_model.bestModel<br />predictions = bestLogisticRegressionModel.transform(test_df)<br />roc_auc = evaluator.evaluate(predictions)<br /><br />print(f"Best ROC AUC: {roc_auc}")<br />for name, coef in zip(feature_columns, bestLogisticRegressionModel.stages[-1].coefficients):<br />    print(f"Feature: {name}, Coefficient: {coef}")<br />print(f"Intercept: {bestLogisticRegressionModel.stages[-1].intercept}")</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/484/1*yxxA6RIo3eQ1yOuY0pE5CA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/483/1*wM2ZzdkUYm0XepLwi9wAIg.png" /></figure><h4><strong>Evaluasi Model</strong></h4><pre># Confusion Matrix<br />pred_df = predictions.select("successful_content", "prediction").toPandas()<br />cm = confusion_matrix(pred_df["successful_content"], pred_df["prediction"])<br />cm_df = pd.DataFrame(cm, index=["Actual 0", "Actual 1"], columns=["Predicted 0", "Predicted 1"])<br /><br /># Visualisasi Confusion Matrix<br />plt.figure(figsize=(8,6))<br />sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")<br />plt.title("Confusion Matrix")<br />plt.xlabel("Predicted")<br />plt.ylabel("Actual")<br />plt.show()<br /><br /># Precision, Recall, dan F1 Score<br />precision = precision_score(pred_df["successful_content"], pred_df["prediction"])<br />recall = recall_score(pred_df["successful_content"], pred_df["prediction"])<br />f1 = f1_score(pred_df["successful_content"], pred_df["prediction"])<br /><br />print(f"Precision: {precision}")<br />print(f"Recall: {recall}")<br />print(f"F1 Score: {f1}")</pre><p>Setelah model dibangun dan dievaluasi, kita akan menampilkan hasil prediksi dan visualisasi yang memberikan wawasan lebih lanjut. Kita akan memvisualisasikan hasil model menggunakan <strong>confusion matrix </strong>untuk memahami bagaimana model membedakan antara acara yang berhasil dan yang gagal.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*KqTiPue8aDnxTWcMTOa87g.png" /></figure><pre>Precision: 0.8482655549437583<br />Recall: 0.9647521918053319<br />F1 Score: 0.902766732242267</pre><h3>Linear Regression untuk Prediksi Rating atau Popularitas</h3><p>Setelah membangun model klasifikasi, kita akan beralih ke <strong>Linear Regression</strong> untuk memprediksi nilai numerik seperti <em>rating</em> atau <em>popularitas</em> acara TV. Linear Regression sangat berguna ketika kita ingin memprediksi angka yang bersifat kontinyu, seperti seberapa populer sebuah acara atau seberapa tinggi ratingnya.</p><p><strong>Langkah-langkah:</strong></p><ul><li><strong>Persiapkan Data</strong>: Tentukan kolom target yang ingin diprediksi, seperti <em>vote_average</em> (rating).</li><li><strong>Latih Model</strong>: Gunakan Linear Regression untuk memprediksi rating berdasarkan fitur lain.</li><li><strong>Evaluasi Model</strong>: Gunakan metrik <strong>RMSE (Root Mean Squared Error)</strong> untuk mengevaluasi kinerja model.</li></ul><p><strong>Kode Program:</strong></p><p>Di bagian ini, kita menggunakan <strong>Spark MLlib</strong> untuk melakukan Linear Regression dan mengukur <strong>RMSE</strong>.</p><h4><strong>Persiapkan Data</strong></h4><pre>from pyspark.sql import SparkSession<br />from pyspark.sql.functions import when<br /><br /># Inisialisasi Spark<br />spark = SparkSession.builder.appName("MLlib Linear Regression").getOrCreate()<br /><br /># Konversi dataframe pandas ke Spark<br />spark_df = spark.createDataFrame(df)<br /><br /># Membuat kolom 'successful_content' untuk klasifikasi sederhana<br />spark_df = spark_df.withColumn("successful_content", when(spark_df.episode_run_time > 50, 1).otherwise(0))<br />from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler<br /><br /># Encoding kolom kategori<br />indexer = StringIndexer(inputCols=["original_language", "type"], outputCols=["original_language_index", "type_index"])<br />indexed_df = indexer.fit(spark_df).transform(spark_df)<br /><br /># Membuat vektor fitur<br />feature_columns = [<br />    "number_of_seasons","number_of_episodes", "vote_count",<br />    "vote_average", "popularity", "episode_run_time",<br />    "original_language_index", "type_index"<br />]<br />assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")<br />assembled_df = assembler.transform(indexed_df)<br /><br /># Normalisasi data<br />scaler = StandardScaler(inputCol="features", outputCol="scaled_features")<br />scaled_df = scaler.fit(assembled_df).transform(assembled_df)</pre><h4><strong>Latih Model</strong></h4><pre>from pyspark.ml.regression import LinearRegression<br />from pyspark.ml.evaluation import RegressionEvaluator<br /><br /># Membagi data<br />train_df, test_df = scaled_df.randomSplit([0.8, 0.2], seed=42)<br /><br /># Membuat model regresi<br />lr = LinearRegression(featuresCol='scaled_features', labelCol='episode_run_time')<br />linearRegressionModel = lr.fit(train_df)<br /><br /># Prediksi pada data uji<br />predictions = linearRegressionModel.transform(test_df)<br /><br /># Evaluasi model<br />evaluator = RegressionEvaluator(labelCol="episode_run_time", predictionCol="prediction", metricName="rmse")<br />rmse = evaluator.evaluate(predictions)<br />print(f"RMSE: {rmse}")<br /><br />evaluator_mae = RegressionEvaluator(labelCol="episode_run_time", predictionCol="prediction", metricName="mae")<br />mae = evaluator_mae.evaluate(predictions)<br />print(f"MAE: {mae}")<br /><br /># Koefisien dan intercept<br />for name, coef in zip(feature_columns, linearRegressionModel.coefficients):<br />    print(f"Feature: {name}, Coefficient: {coef}")<br />print(f'Intercept: {linearRegressionModel.intercept}')<br />from pyspark.ml.tuning import CrossValidator, ParamGridBuilder<br /><br /># Menyusun grid parameter<br />paramGrid = ParamGridBuilder() \<br />    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \<br />    .addGrid(lr.maxIter, [10, 50, 100, 200]) \<br />    .build()<br /><br /># Cross Validator<br />crossval = CrossValidator(estimator=lr,<br />                          estimatorParamMaps=paramGrid,<br />                          evaluator=RegressionEvaluator(labelCol="episode_run_time", metricName="rmse"),<br />                          numFolds=5)<br /><br /># Melatih model dengan cross-validation<br />model = crossval.fit(train_df)<br /><br /># Model terbaik<br />best_model = model.bestModel<br /><br /># Evaluasi ulang<br />predictions = best_model.transform(test_df)<br />rmse = RegressionEvaluator(labelCol="episode_run_time", metricName="rmse").evaluate(predictions)<br />mae = RegressionEvaluator(labelCol="episode_run_time", metricName="mae").evaluate(predictions)<br /><br />print(f"RMSE: {rmse}")<br />print(f"MAE: {mae}")<br />for name, coef in zip(feature_columns, best_model.coefficients):<br />    print(f"Feature: {name}, Coefficient: {coef}")<br />print(f"Best Model Intercept: {best_model.intercept}")</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/536/1*cZz2iJvVGSzZ8gR-tZqHOA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/535/1*ILWqKE1IFjZeeMRRMBnAAA.png" /></figure><h4><strong>Evaluasi Model</strong></h4><pre>import matplotlib.pyplot as plt<br />import seaborn as sns<br />from pyspark.ml.evaluation import RegressionEvaluator<br /><br /># Mengambil prediksi dari model<br />predictions_pd = predictions.toPandas()<br /><br /># Hitung residuals<br />predictions_pd['residuals'] = predictions_pd['episode_run_time'] - predictions_pd['prediction']<br /><br /># Membuat figure dengan dua subplots<br />fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))<br /><br /># Visualisasi Predicted vs Actual<br />sns.scatterplot(x='prediction', y='episode_run_time', data=predictions_pd, alpha=0.5, color="blue", ax=ax1)<br />ax1.plot([min(predictions_pd['prediction']), max(predictions_pd['prediction'])],<br />         [min(predictions_pd['episode_run_time']), max(predictions_pd['episode_run_time'])],<br />         linestyle='--', color='red', linewidth=2)<br />ax1.set_title('Predicted vs Actual Episode Run Time', fontsize=16)<br />ax1.set_xlabel('Predicted Episode Run Time', fontsize=12)<br />ax1.set_ylabel('Actual Episode Run Time', fontsize=12)<br />ax1.grid(True, linestyle='--', alpha=0.7)<br /><br /># Visualisasi residuals<br />sns.scatterplot(x='prediction', y='residuals', data=predictions_pd, alpha=0.5, color="purple", ax=ax2)<br /># ax2.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error Line')<br />ax2.set_title('Residuals vs Predicted Values', fontsize=16)<br />ax2.set_xlabel('Predicted Episode Run Time', fontsize=12)<br />ax2.set_ylabel('Residuals', fontsize=12)<br />ax2.legend()<br />ax2.grid(True, linestyle='--', alpha=0.7)<br /><br />plt.tight_layout()<br />plt.show()</pre><p>Setelah model dibangun dan dievaluasi, kita akan menampilkan hasil prediksi dan visualisasi yang memberikan wawasan lebih lanjut. Kita akan memvisualisasikan hasil model menggunakan <strong>scatter plot </strong>untuk visualisasi distribusi hasil prediksi.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5iRkgReUdKQddQ6k7Krcwg.png" /></figure><h3>Membandingkan Logistic Regression dan Linear Regression</h3><p>Setelah membangun kedua model tersebut, mari kita bandingkan kinerja dari <strong>Logistic Regression</strong> dan <strong>Linear Regression</strong>. Keduanya memiliki tujuan yang berbeda:</p><ul><li><strong>Logistic Regression</strong> digunakan untuk klasifikasi biner, misalnya memprediksi apakah acara TV akan sukses atau tidak.</li><li><strong>Linear Regression</strong> digunakan untuk memprediksi nilai kontinu, seperti rating atau popularitas acara TV.</li></ul><p>Untuk perbandingan, kita akan menggunakan <strong>AUC</strong> untuk Logistic Regression dan <strong>RMSE</strong> untuk Linear Regression sebagai metrik evaluasi.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/483/1*wM2ZzdkUYm0XepLwi9wAIg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/535/1*ILWqKE1IFjZeeMRRMBnAAA.png" /></figure><h3>Kesimpulan</h3><p>Pada akhirnya, dengan menggunakan <strong>Logistic Regression</strong> dan <strong>Linear Regression</strong>, kita dapat membangun dua jenis model untuk menganalisis keberhasilan acara TV:</p><ol><li><strong>Logistic Regression</strong>: Model ini berguna untuk memprediksi apakah acara TV akan sukses atau gagal (klasifikasi biner).</li><li><strong>Linear Regression</strong>: Model ini berguna untuk memprediksi nilai numerik seperti rating atau popularitas acara TV.</li></ol><p>Dengan dua pendekatan ini, Anda bisa mendapatkan pemahaman yang lebih baik tentang keberhasilan konten TV, baik dalam konteks klasifikasi (sukses atau tidak) maupun dalam prediksi numerik (rating atau popularitas).</p><ul><li><a href="https://colab.research.google.com/drive/12uipm7ZW6v06D94R9BlJLQD3_9pO0gEp?usp=sharing">Google Colab</a></li><li><a href="https://www.kaggle.com/datasets/asaniczka/full-tmdb-tv-shows-dataset-2023-150k-shows">Full TMDb TV Shows Dataset 2024 (150K Shows)</a></li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7b6826951b50" width="1" />
<!--END_SECTION:medium-->
