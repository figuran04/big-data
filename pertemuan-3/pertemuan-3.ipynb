{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nama: Dika Elsaputra\n",
    "\n",
    "NPM: 2320506032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tugas 1**: Pelajari bagaimana MapReduce bekerja dengan dataset sederhana dan coba implementasikan konsep key-value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagaimana MapReduce Bekerja\n",
    "\n",
    "1. **Map Phase**: \n",
    "   - **Input**: Dataset dibagi menjadi bagian-bagian kecil yang disebut splits.\n",
    "   - **Proses**: Fungsi *Map* dipanggil untuk setiap split. Fungsi ini menghasilkan pasangan kunci-nilai (key-value pairs) dari data input. Misalnya, dalam kasus analisis teks, pasangan kunci-nilai bisa berupa kata sebagai kunci dan angka 1 sebagai nilai.\n",
    "   - **Output**: Pasangan kunci-nilai ini kemudian dikelompokkan oleh kunci.\n",
    "\n",
    "2. **Shuffle and Sort Phase**:\n",
    "   - **Pengelompokan**: Pasangan kunci-nilai yang dihasilkan dari fase Map dikelompokkan berdasarkan kunci.\n",
    "   - **Sort**: Pasangan kunci-nilai diurutkan berdasarkan kunci.\n",
    "\n",
    "3. **Reduce Phase**:\n",
    "   - **Input**: Fase ini menerima pasangan kunci-nilai yang telah dikelompokkan dan diurutkan.\n",
    "   - **Proses**: Fungsi *Reduce* dijalankan untuk setiap kelompok kunci. Fungsi ini menggabungkan nilai-nilai terkait dengan kunci yang sama.\n",
    "   - **Output**: Hasil akhir dari fase Reduce adalah output akhir dari proses MapReduce.\n",
    "\n",
    "### Contoh Implementasi Key-Value Pair\n",
    "\n",
    "Mari kita lihat implementasi sederhana dari MapReduce dengan dataset teks kecil. Misalkan kita memiliki teks sebagai berikut dan ingin menghitung jumlah kemunculan setiap kata.\n",
    "\n",
    "**Dataset Input:**\n",
    "```\n",
    "hello world\n",
    "hello MapReduce\n",
    "hello MapReduce world\n",
    "```\n",
    "\n",
    "#### Fungsi Map\n",
    "\n",
    "Fungsi Map akan mengambil setiap baris teks dan menghasilkan pasangan kunci-nilai untuk setiap kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(text):\n",
    "    results = []\n",
    "    for word in text.split():\n",
    "        results.append((word, 1))\n",
    "    return results\n",
    "\n",
    "input_text = \"hello world hello MapReduce hello MapReduce world\"\n",
    "mapped_results = map_function(input_text)\n",
    "print(mapped_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi Reduce\n",
    "\n",
    "Fungsi Reduce akan menggabungkan nilai-nilai yang memiliki kunci yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def reduce_function(pairs):\n",
    "    reduced_result = defaultdict(int)\n",
    "    for key, value in pairs:\n",
    "        reduced_result[key] += value\n",
    "    return dict(reduced_result)\n",
    "\n",
    "reduced_results = reduce_function(mapped_results)\n",
    "print(reduced_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penjelasan\n",
    "\n",
    "1. **Map Function**: Mengambil baris teks dan membagi setiap kata menjadi pasangan kunci-nilai dengan nilai 1.\n",
    "2. **Reduce Function**: Mengumpulkan dan menjumlahkan nilai-nilai yang memiliki kunci yang sama untuk memberikan jumlah total kemunculan setiap kata.\n",
    "\n",
    "Proses MapReduce ini akan memproses data besar dengan cara terdistribusi dan paralel, yang sangat berguna untuk analisis data besar dalam skala besar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tugas 2**: Implementasikan fungsi `map_function` dan `reduce_function` pada dataset teks sederhana, lalu hitung jumlah kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(text):\n",
    "  for word in text.split():\n",
    "    yield (word, 1)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def reduce_function(pairs):\n",
    "  result = defaultdict(int)\n",
    "  for word, count in pairs:\n",
    "    result[word] += count\n",
    "  return result\n",
    "\n",
    "text = \"hello world hello\"\n",
    "mapped = list(map_function(text))\n",
    "result = dict(reduce_function(mapped))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tugas 3**: Upload file teks ke HDFS, jalankan perintah MapReduce Word Count, dan tampilkan hasilnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```batch\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -mkdir /user/figuran04\n",
    "hdfs dfs -mkdir /user/figuran04/input\n",
    "cd Downloads\n",
    "hdfs dfs -put input.txt /user/figuran04/input/\n",
    "hadoop jar C:/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/figuran04/input/ /user/figuran04/output/\n",
    "hdfs dfs -cat /user/figuran04/output/part-r-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```batch\n",
    "Duis    1\n",
    "Excepteur       1\n",
    "Lorem   1\n",
    "Ut      1\n",
    "ad      1\n",
    "adipiscing      1\n",
    "aliqua. 1\n",
    "aliquip 1\n",
    "amet,   1\n",
    "anim    1\n",
    "aute    1\n",
    "cillum  1\n",
    "commodo 1\n",
    "consectetur     1\n",
    "consequat.      1\n",
    "culpa   1\n",
    "cupidatat       1\n",
    "deserunt        1\n",
    "do      1\n",
    "dolor   2\n",
    "dolore  2\n",
    "ea      1\n",
    "eiusmod 1\n",
    "elit,   1\n",
    "enim    1\n",
    "esse    1\n",
    "est     1\n",
    "et      1\n",
    "eu      1\n",
    "ex      1\n",
    "exercitation    1\n",
    "fugiat  1\n",
    "id      1\n",
    "in      3\n",
    "incididunt      1\n",
    "ipsum   1\n",
    "irure   1\n",
    "labore  1\n",
    "laboris 1\n",
    "laborum.        1\n",
    "magna   1\n",
    "minim   1\n",
    "mollit  1\n",
    "nisi    1\n",
    "non     1\n",
    "nostrud 1\n",
    "nulla   1\n",
    "occaecat        1\n",
    "officia 1\n",
    "pariatur.       1\n",
    "proident,       1\n",
    "qui     1\n",
    "quis    1\n",
    "reprehenderit   1\n",
    "sed     1\n",
    "sint    1\n",
    "sit     1\n",
    "sunt    1\n",
    "tempor  1\n",
    "ullamco 1\n",
    "ut      2\n",
    "velit   1\n",
    "veniam, 1\n",
    "voluptate       1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tugas 4**: Cari dataset besar, jalankan MapReduce untuk menghitung kata, dan buat laporan analisis hasil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **File System Counters**\n",
    "- **FILE: Number of bytes read=6925**  \n",
    "  Mengacu pada jumlah total byte yang dibaca oleh sistem file lokal selama eksekusi. Ini termasuk data yang dibaca selama fase Map dan Reduce dari disk lokal.\n",
    "  \n",
    "- **FILE: Number of bytes written=569429**  \n",
    "  Ini adalah total byte yang ditulis ke sistem file lokal selama eksekusi tugas. Hasil ini menunjukkan besar output yang dihasilkan pada node lokal setelah seluruh proses pemrosesan.\n",
    "\n",
    "- **HDFS: Number of bytes read=13621**  \n",
    "  Mengacu pada total byte yang dibaca dari HDFS, baik pada fase Map maupun Reduce. Data ini termasuk input yang diberikan untuk Map dan output antar fase dari Map ke Reduce.\n",
    "\n",
    "- **HDFS: Number of bytes written=6151**  \n",
    "  Jumlah byte yang ditulis ke HDFS sebagai hasil akhir dari proses Reduce. Nilai ini biasanya lebih kecil dibandingkan input, karena data sudah terproses dan direduksi.\n",
    "\n",
    "- **HDFS: Number of read/write operations**  \n",
    "  Terdapat 8 operasi pembacaan dan 2 operasi penulisan di HDFS selama eksekusi, menunjukkan aktivitas I/O pada cluster HDFS selama proses MapReduce.\n",
    "\n",
    "#### 2. **Job Counters**\n",
    "- **Launched map tasks=1, Launched reduce tasks=1**  \n",
    "  Menunjukkan bahwa hanya satu tugas Map dan satu tugas Reduce yang diluncurkan untuk proses ini, karena dataset relatif kecil.\n",
    "\n",
    "- **Data-local map tasks=1**  \n",
    "  Mengonfirmasi bahwa tugas Map diluncurkan pada node yang sama dengan tempat data disimpan, meningkatkan efisiensi dengan meminimalkan transfer data di jaringan.\n",
    "\n",
    "- **Total time spent by all map tasks (ms)=17091, Total time spent by all reduce tasks (ms)=18621**  \n",
    "  Waktu total yang dihabiskan oleh tugas Map dan Reduce untuk menyelesaikan eksekusi. Map membutuhkan waktu 17091 ms, sedangkan Reduce memerlukan waktu 18621 ms. Ini mengindikasikan bahwa sebagian besar waktu dihabiskan di fase Reduce.\n",
    "\n",
    "- **Total megabyte-milliseconds taken by all map tasks=17501184**  \n",
    "  Jumlah total megabyte yang diproses oleh tugas Map dalam milidetik. Angka ini membantu mengukur throughput pemrosesan data selama fase Map.\n",
    "\n",
    "#### 3. **Map-Reduce Framework**\n",
    "- **Map input records=88**  \n",
    "  Sebanyak 88 catatan yang diproses oleh tugas Map dari dataset.\n",
    "\n",
    "- **Map output records=721**  \n",
    "  Setelah diproses, Map menghasilkan 721 record output. Hal ini disebabkan oleh pemecahan dataset ke dalam unit key-value yang lebih kecil.\n",
    "\n",
    "- **Reduce input records=196, Reduce output records=196**  \n",
    "  Fase Reduce menerima 196 record dan menghasilkan jumlah yang sama setelah reduksi. Hal ini menunjukkan proses penggabungan key-value yang efisien tanpa ada penghilangan data.\n",
    "\n",
    "- **Spilled Records=392**  \n",
    "  Record yang dituliskan ke disk selama proses Shuffle (saat output dari Map dipindahkan ke Reduce). Semakin besar dataset, semakin banyak catatan yang akan tertumpah ke disk selama proses Shuffle.\n",
    "\n",
    "- **Shuffle Errors (BAD_ID, CONNECTION, IO_ERROR, dll.)**  \n",
    "  Tidak ada error yang terjadi selama proses shuffle, yang berarti seluruh transfer data antar node berjalan lancar.\n",
    "\n",
    "#### 4. **File Input Format Counters**\n",
    "- **Bytes Read=13493**  \n",
    "  Ukuran data yang dibaca oleh framework dari HDFS selama fase Map, menunjukkan total byte dari file input yang diproses.\n",
    "\n",
    "#### 5. **File Output Format Counters**\n",
    "- **Bytes Written=6151**  \n",
    "  Jumlah byte yang ditulis ke HDFS sebagai output akhir dari tugas Reduce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tugas Tambahan**: Buat algoritma MapReduce lainnya, seperti menghitung rata-rata nilai, atau menghitung frekuensi kemunculan elemen tertentu di dalam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function_combined(data):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for element in data:\n",
    "        total += element if isinstance(element, (int, float)) else 0\n",
    "        count += 1 if isinstance(element, (int, float)) else 0\n",
    "        yield ('frequency', (element, 1))\n",
    "    yield ('average', (total, count))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def reduce_function_combined(pairs):\n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    frequency_result = defaultdict(int)\n",
    "    for key, value in pairs:\n",
    "        if key == 'average':\n",
    "            partial_sum, partial_count = value\n",
    "            total_sum += partial_sum\n",
    "            total_count += partial_count\n",
    "        elif key == 'frequency':\n",
    "            element, count = value\n",
    "            frequency_result[element] += count\n",
    "    average = total_sum / total_count if total_count > 0 else 0\n",
    "    return {'average': average, 'frequency': dict(frequency_result)}\n",
    "\n",
    "data = [10, 20, 'apple', 30, 'banana', 'apple', 40, 'banana', 'apple', 50]\n",
    "mapped = map_function_combined(data)  \n",
    "reduced = reduce_function_combined(mapped)  \n",
    "print(f\"Hasil Rata-rata dan Frekuensi Kemunculan Elemen: {reduced}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
