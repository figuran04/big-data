<!--START_SECTION:medium-->
<h3>Implementasi MapReduce Hadoop Sederhana: Python hingga Hadoop Dataset Besar</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/740/1*EDZMmI15hqMc5gyH8OgZlA.png" /></figure><p>MapReduce adalah model pemrograman yang dirancang untuk memproses data dalam skala besar secara paralel menggunakan banyak node pada cluster. Konsep utamanya adalah memecah (map) data menjadi pasangan key-value dan menggabungkan (reduce) hasilnya untuk memperoleh hasil akhir. MapReduce sangat berguna dalam pemrosesan data masif seperti menghitung jumlah kata atau menganalisis dataset besar.</p><p><strong><em>Bagaimana MapReduce Bekerja dengan Dataset Sederhana?</em></strong></p><p>MapReduce bekerja dalam dua fase utama: fase Map dan fase Reduce. Pada fase Map, data masukan dipecah menjadi pasangan key-value. Pada fase Reduce, pasangan key-value tersebut digabungkan untuk mendapatkan hasil akhir.</p><p>Mari kita lihat implementasi sederhana dari MapReduce menggunakan contoh perhitungan jumlah kata (word count).</p><h3>Implementasi Sederhana: Word Count</h3><ul><li><strong>Map Function:</strong></li></ul><p>Fungsi Map berfungsi untuk memecah teks menjadi kata-kata individu dan mengeluarkan pasangan key-value di mana setiap kata diberikan nilai 1.</p><pre>def map_function(text):<br />  for word in text.split():<br />    yield (word, 1)</pre><ul><li><strong>Reduce Function:</strong></li></ul><p>Fungsi Reduce menerima keluaran dari fungsi Map dan menggabungkannya untuk menghitung frekuensi kemunculan setiap kata.</p><pre>from collections import defaultdict<br /><br />def reduce_function(pairs):<br />  result = defaultdict(int)<br />  for word, count in pairs:<br />    result[word] += count<br />  return result</pre><ul><li><strong>Implementasi Lengkap:</strong></li></ul><pre>def map_function(text):<br />  for word in text.split():<br />    yield (word, 1)<br /><br />from collections import defaultdict<br /><br />def reduce_function(pairs):<br />  result = defaultdict(int)<br />  for word, count in pairs:<br />    result[word] += count<br />  return result<br /><br />text = "hello world hello"<br />mapped = list(map_function(text))<br />result = dict(reduce_function(mapped))<br />print(result)</pre><ul><li><strong>Hasil:</strong></li></ul><pre>{'hello': 2, 'world': 1}</pre><h3>Menjalankan Word Count di Hadoop</h3><p>Sebelum mengikuti sesi ini/belum instalasi hadoop diharapkan kembali ke <a href="https://medium.com/@dikaelsaputra/instalasi-dan-konfigurasi-hadoop-serta-spark-di-windows-f7f3582def93">artikel ini</a>.</p><p>Untuk menjalankan MapReduce di Hadoop, ikuti langkah-langkah berikut:</p><ul><li><strong>Siapkan</strong> file teks yang akan dihitung jumlah katanya.<br />Buat file input.txt di direktori Downloads dengan isi teks yang ingin diproses.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/606/1*rax06PXEkLtCoE_RvuC6Eg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*3AN1kEetYDGIBIM3Ki1BmQ.png" /></figure><ul><li><strong>Buka </strong>Command Promt <strong>Run as Administrator.</strong></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*O12OQrJzJNd5DRKlg1m-5A.png" /></figure><ul><li><strong>Nyalakan </strong>hadoop dengan cara ketik start-all lalu Enter. Pastikan semua berjalan dengan benar dengan cara ketik jps .</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*2XOFA63WYMHEQzOTKGjwkQ.png" /></figure><ul><li><strong>Masukkan</strong> file ke dalam HDFS dan jalankan MapReduce untuk menghitung jumlah kata.</li></ul><pre>hdfs dfs -mkdir /user<br />hdfs dfs -mkdir /user/figuran04<br />hdfs dfs -mkdir /user/figuran04/input<br />cd Downloads<br />hdfs dfs -put input.txt /user/figuran04/input/<br />hadoop jar C:/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/figuran04/input/ /user/figuran04/output/</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*jCE4YAVJAIZdUVGuMEHqVA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*zRLCPVjlZEGw3nFsgLyusQ.png" /></figure><ul><li><strong>Tampilkan</strong> hasil perhitungan kata.</li></ul><pre>hdfs dfs -cat /user/figuran04/output/part-r-00000</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*mCvOBG8KtbowZcOEgdq2fA.png" /></figure><h3>Menganalisis Dataset Besar</h3><p>Selain menggunakan dataset sederhana, kita juga dapat menjalankan MapReduce pada dataset besar seperti yang bisa diunduh dari <a href="https://www.kaggle.com/">Kaggle</a>. Misalnya, dataset <a href="https://www.kaggle.com/datasets/abdullahashfaqvirk/student-mental-health-survey"><strong>Mental Health Survey</strong></a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*quZbfmiLpIKsoKmZwG8mlw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*hkSQHdL1-D2EIGO-fZZyvg.png" /></figure><ul><li><strong>Masukkan</strong> dataset besar ke HDFS dan jalankan Word Count pada dataset besar tersebut.</li></ul><pre>hdfs dfs -put MentalHealthSurvey.csv /user/figuran04/input/<br />hadoop jar C:/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/figuran04/input/MentalHealthSurvey.csv /user/figuran04/output-csv/</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*qp4Ykj3qJuWeIU_rnoYDPQ.png" /></figure><ul><li><strong>Tampilkan</strong> hasil Word Count dari dataset besar.</li></ul><pre>hdfs dfs -cat /user/figuran04/output-csv/part-r-00000</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/827/1*C4cZi4oIwwW-3SE0rWsckA.png" /></figure><h4><strong>Analisis </strong>Hasil MapReduce</h4><p>Hasil dari eksekusi MapReduce untuk dataset besar memberikan berbagai metrik yang dapat dianalisis, seperti jumlah byte yang dibaca/ditulis, performa tugas Map dan Reduce, serta status framework yang menangani pemrosesan data. Berikut adalah analisis dari masing-masing bagian hasil.</p><p><strong>File System Counters</strong></p><ul><li><strong>FILE: Number of bytes read=6925</strong></li></ul><p>Mengacu pada jumlah total byte yang dibaca oleh sistem file lokal selama eksekusi. Ini termasuk data yang dibaca selama fase Map dan Reduce dari disk lokal.</p><ul><li><strong>FILE: Number of bytes written=569429</strong></li></ul><p>Ini adalah total byte yang ditulis ke sistem file lokal selama eksekusi tugas. Hasil ini menunjukkan besar output yang dihasilkan pada node lokal setelah seluruh proses pemrosesan.</p><ul><li><strong>HDFS: Number of bytes read=13621</strong></li></ul><p>Mengacu pada total byte yang dibaca dari HDFS, baik pada fase Map maupun Reduce. Data ini termasuk input yang diberikan untuk Map dan output antar fase dari Map ke Reduce.</p><ul><li><strong>HDFS: Number of bytes written=6151</strong></li></ul><p>Jumlah byte yang ditulis ke HDFS sebagai hasil akhir dari proses Reduce. Nilai ini biasanya lebih kecil dibandingkan input, karena data sudah terproses dan direduksi.</p><ul><li><strong>HDFS: Number of read/write operations</strong></li></ul><p>Terdapat 8 operasi pembacaan dan 2 operasi penulisan di HDFS selama eksekusi, menunjukkan aktivitas I/O pada cluster HDFS selama proses MapReduce.</p><p><strong>Job Counters</strong></p><ul><li><strong>Launched map tasks=1, Launched reduce tasks=1</strong></li></ul><p>Menunjukkan bahwa hanya satu tugas Map dan satu tugas Reduce yang diluncurkan untuk proses ini, karena dataset relatif kecil.</p><ul><li><strong>Data-local map tasks=1</strong></li></ul><p>Mengonfirmasi bahwa tugas Map diluncurkan pada node yang sama dengan tempat data disimpan, meningkatkan efisiensi dengan meminimalkan transfer data di jaringan.</p><ul><li><strong>Total time spent by all map tasks (ms)=17091, Total time spent by all reduce tasks (ms)=18621</strong></li></ul><p>Waktu total yang dihabiskan oleh tugas Map dan Reduce untuk menyelesaikan eksekusi. Map membutuhkan waktu 17091 ms, sedangkan Reduce memerlukan waktu 18621 ms. Ini mengindikasikan bahwa sebagian besar waktu dihabiskan di fase Reduce.</p><ul><li><strong>Total megabyte-milliseconds taken by all map tasks=17501184</strong><br />Jumlah total megabyte yang diproses oleh tugas Map dalam milidetik. Angka ini membantu mengukur throughput pemrosesan data selama fase Map.</li></ul><p><strong>Map-Reduce Framework</strong></p><ul><li><strong>Map input records=88</strong></li></ul><p>Sebanyak 88 catatan yang diproses oleh tugas Map dari dataset.</p><ul><li><strong>Map output records=721</strong></li></ul><p>Setelah diproses, Map menghasilkan 721 record output. Hal ini disebabkan oleh pemecahan dataset ke dalam unit key-value yang lebih kecil.</p><ul><li><strong>Reduce input records=196, Reduce output records=196</strong></li></ul><p>Fase Reduce menerima 196 record dan menghasilkan jumlah yang sama setelah reduksi. Hal ini menunjukkan proses penggabungan key-value yang efisien tanpa ada penghilangan data.</p><ul><li><strong>Spilled Records=392</strong></li></ul><p>Record yang dituliskan ke disk selama proses Shuffle (saat output dari Map dipindahkan ke Reduce). Semakin besar dataset, semakin banyak catatan yang akan tertumpah ke disk selama proses Shuffle.</p><p><strong>Shuffle Errors (BAD_ID, CONNECTION, IO_ERROR, dll.)</strong></p><p>Tidak ada error yang terjadi selama proses shuffle, yang berarti seluruh transfer data antar node berjalan lancar.</p><p><strong>File Input Format Counters</strong></p><ul><li><strong>Bytes Read=13493</strong></li></ul><p>Ukuran data yang dibaca oleh framework dari HDFS selama fase Map, menunjukkan total byte dari file input yang diproses.</p><p><strong>File Output Format Counters</strong></p><ul><li><strong>Bytes Written=6151</strong></li></ul><p>Jumlah byte yang ditulis ke HDFS sebagai output akhir dari tugas Reduce.</p><h4><strong>Custom</strong> MapReduce Algorithm</h4><pre>def map_function_combined(data):<br />    total = 0<br />    count = 0<br />    for element in data:<br />        total += element if isinstance(element, (int, float)) else 0<br />        count += 1 if isinstance(element, (int, float)) else 0<br />        yield ('frequency', (element, 1))<br />    yield ('average', (total, count))<br /><br />from collections import defaultdict<br /><br />def reduce_function_combined(pairs):<br />    total_sum = 0<br />    total_count = 0<br />    frequency_result = defaultdict(int)<br />    for key, value in pairs:<br />        if key == 'average':<br />            partial_sum, partial_count = value<br />            total_sum += partial_sum<br />            total_count += partial_count<br />        elif key == 'frequency':<br />            element, count = value<br />            frequency_result[element] += count<br />    average = total_sum / total_count if total_count > 0 else 0<br />    return {'average': average, 'frequency': dict(frequency_result)}<br /><br />data = [10, 20, 'apple', 30, 'banana', 'apple', 40, 'banana', 'apple', 50]<br />mapped = map_function_combined(data)  <br />reduced = reduce_function_combined(mapped)  <br />print(f"Hasil Rata-rata dan Frekuensi Kemunculan Elemen: {reduced}")</pre><ul><li><strong>Hasil:</strong></li></ul><pre>Hasil Rata-rata dan Frekuensi Kemunculan Elemen: {<br />  'average': 30.0, <br />  'frequency': {<br />    10: 1, 20: 1, <br />    'apple': 3, <br />    30: 1, <br />    'banana': 2, <br />    40: 1, <br />    50: 1<br />  }<br />}</pre><h3>Kesimpulan</h3><p>Dengan menggunakan MapReduce, kita dapat memproses data secara efisien pada skala besar. Contoh Word Count sederhana yang diterapkan di atas menunjukkan bagaimana fungsi Map dan Reduce bekerja bersama untuk menghasilkan hasil akhir. Selain itu, Hadoop memungkinkan kita untuk memproses dataset besar dengan sangat mudah di lingkungan terdistribusi, memanfaatkan cluster untuk mempercepat pemrosesan data.</p><h3>Referensi</h3><ul><li><a href="https://www.kaggle.com/datasets/abdullahashfaqvirk/student-mental-health-survey">Student Mental Health Survey</a></li><li><a href="https://drive.google.com/file/d/1zta5ZPq0dLv5EzaW5zCLINyh4BP5eN76/view?usp=sharing">Hands_On_Pertemuan_3.ipynb</a></li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b413cfc67f5c" width="1" />
<!--END_SECTION:medium-->